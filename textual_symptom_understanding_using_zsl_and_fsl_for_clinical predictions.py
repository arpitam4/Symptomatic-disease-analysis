# -*- coding: utf-8 -*-
"""Textual Symptom Understanding using ZSL and FSL for Clinical¬†Predictions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qwsQdoAGb33mL4CeiD3QCIA3npRm-4JV

#BIOBert

##ZSL

###DS1
"""

import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel
from datasets import load_dataset
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# Load dataset
ds = load_dataset("fhai50032/Symptoms_to_disease_7k")
df = pd.DataFrame(ds['train'])

# Extract symptom text and disease labels
df['symptoms'] = df['query'].apply(lambda x: x.split('Patient:')[1].strip())
df['disease'] = df['response'].apply(lambda x: x.replace("You may have ", "").replace(".", ""))

# Sample a subset to speed up
df = df.sample(1000, random_state=42).reset_index(drop=True)

# Load BioBERT
model_name = "dmis-lab/biobert-v1.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model.eval()

# Function to get BioBERT CLS embeddings
def get_cls_embedding(text):
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
        outputs = model(**inputs)
        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

# Get unique disease labels
disease_labels = df['disease'].unique()
print(f"Total unique disease labels: {len(disease_labels)}")

# Compute embeddings for all disease labels
print("Encoding disease label embeddings...")
disease_embeddings = {label: get_cls_embedding(label) for label in tqdm(disease_labels)}

# Prepare ZSL test data
test_df = df.sample(300, random_state=123)
print(f"Test samples: {len(test_df)}")

# Predict using cosine similarity
correct = 0
preds = []
for _, row in tqdm(test_df.iterrows(), total=len(test_df)):
    symptom = row['symptoms']
    true_label = row['disease']

    symptom_emb = get_cls_embedding(symptom)
    similarities = {label: cosine_similarity([symptom_emb], [emb])[0][0] for label, emb in disease_embeddings.items()}
    predicted_label = max(similarities, key=similarities.get)

    preds.append((true_label, predicted_label))
    if predicted_label == true_label:
        correct += 1

# Compute accuracy
accuracy = correct / len(test_df)
print(f"\nüîç BioBERT Zero-Shot Accuracy: {accuracy:.2%}")

"""###DS2"""

import pandas as pd
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import re
import torch

# Load dataset
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

# Extract symptoms-to-disease samples
processed_data = []
for _, row in df.iterrows():
    instr, out = row['instruction'], row['output']
    if "I am having the following symptoms:" in instr and "What could be the disease" in instr:
        symptoms_match = re.search(r'I am having the following symptoms: (.*?)(?:\.|$)', instr)
        disease_match = re.search(r'dealing with (.*?)(?:\.|$)', out)
        if symptoms_match and disease_match:
            processed_data.append({
                "symptoms": symptoms_match.group(1).strip(),
                "disease": disease_match.group(1).strip()
            })

df_proc = pd.DataFrame(processed_data)
print(f"Processed dataset size: {len(df_proc)}")

# Limit to top 20 diseases
top_diseases = df_proc["disease"].value_counts().head(20).index.tolist()
df_proc = df_proc[df_proc["disease"].isin(top_diseases)].reset_index(drop=True)

# Split
train_df, test_df = train_test_split(df_proc, test_size=0.2, random_state=42)
print(f"Test set size: {len(test_df)}")

# Load BioBERT
model_name = "dmis-lab/biobert-v1.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)
model.eval()

# Embedding function
def get_embedding(text):
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
        outputs = model(**inputs)
        return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

# Encode disease labels
print("Encoding disease label embeddings...")
disease_embs = {d: get_embedding(d) for d in tqdm(top_diseases)}

# Predict function
def predict(symptom):
    s_emb = get_embedding(symptom)
    sims = {d: cosine_similarity([s_emb], [emb])[0][0] for d, emb in disease_embs.items()}
    return max(sims, key=sims.get)

# Run predictions
print("Running predictions...")
test_df["predicted"] = [predict(sym) for sym in tqdm(test_df["symptoms"])]

# Accuracy
acc = (test_df["predicted"] == test_df["disease"]).mean()
print(f"\nüîç BioBERT Zero-Shot Accuracy (Top-20 classes): {acc:.2%}")

# Sample output
print("\nSample Predictions:")
print(test_df[["symptoms", "disease", "predicted"]].head(10))

"""###DS3"""

!pip install transformers datasets scikit-learn tqdm

from datasets import load_dataset
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm
import torch

# Load dataset
print("Loading dataset...")
ds = load_dataset("Mohamed-Ahmed161/Disease-Symptoms")
df = pd.DataFrame(ds['train'])

# Clean data
df = df[['Disease_Name', 'Generated_Sentence_From_symptoms']].dropna()
df = df.rename(columns={'Disease_Name': 'disease', 'Generated_Sentence_From_symptoms': 'symptoms'})
df = df.drop_duplicates()
print(f"Dataset size: {len(df)}")

# Load BioBERT
print("Loading BioBERT...")
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
model = AutoModel.from_pretrained("dmis-lab/biobert-v1.1")
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Embedding function
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()
    return emb

# Sample for speed (optional)
df = df.sample(500, random_state=42).reset_index(drop=True)

# Get unique diseases for candidate labels
disease_labels = df['disease'].unique()
print(f"Number of candidate diseases: {len(disease_labels)}")

# Precompute disease embeddings
print("Generating embeddings for disease labels...")
disease_embeddings = {d: get_embedding(d) for d in tqdm(disease_labels)}

# Compute predictions using cosine similarity
preds = []
true = []

print("Running zero-shot inference...")
for _, row in tqdm(df.iterrows(), total=len(df)):
    symptom_emb = get_embedding(row['symptoms'])
    similarities = {d: cosine_similarity([symptom_emb], [emb])[0][0] for d, emb in disease_embeddings.items()}
    predicted_disease = max(similarities, key=similarities.get)
    preds.append(predicted_disease)
    true.append(row['disease'])

# Evaluation
accuracy = accuracy_score(true, preds)
precision = precision_score(true, preds, average='weighted', zero_division=0)
recall = recall_score(true, preds, average='weighted', zero_division=0)
f1 = f1_score(true, preds, average='weighted', zero_division=0)

print("\nBioBERT Zero-Shot Learning Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision (weighted): {precision:.4f}")
print(f"Recall (weighted): {recall:.4f}")
print(f"F1 Score (weighted): {f1:.4f}")

"""###DS4"""

!pip install transformers datasets scikit-learn tqdm

from datasets import load_dataset
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm
import torch

# Load dataset
print("Loading dataset...")
ds = load_dataset("Mohamed-Ahmed161/Disease-Symptoms")
df = pd.DataFrame(ds['train'])

# Clean data
df = df[['Disease_Name', 'Generated_Sentence_From_symptoms']].dropna()
df = df.rename(columns={'Disease_Name': 'disease', 'Generated_Sentence_From_symptoms': 'symptoms'})
df = df.drop_duplicates()
print(f"Dataset size: {len(df)}")

# Load BioBERT
print("Loading BioBERT...")
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
model = AutoModel.from_pretrained("dmis-lab/biobert-v1.1")
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Embedding function
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()
    return emb

# Sample for speed (optional)
df = df.sample(500, random_state=42).reset_index(drop=True)

# Get unique diseases for candidate labels
disease_labels = df['disease'].unique()
print(f"Number of candidate diseases: {len(disease_labels)}")

# Precompute disease embeddings
print("Generating embeddings for disease labels...")
disease_embeddings = {d: get_embedding(d) for d in tqdm(disease_labels)}

# Compute predictions using cosine similarity
preds = []
true = []

print("Running zero-shot inference...")
for _, row in tqdm(df.iterrows(), total=len(df)):
    symptom_emb = get_embedding(row['symptoms'])
    similarities = {d: cosine_similarity([symptom_emb], [emb])[0][0] for d, emb in disease_embeddings.items()}
    predicted_disease = max(similarities, key=similarities.get)
    preds.append(predicted_disease)
    true.append(row['disease'])

# Evaluation
accuracy = accuracy_score(true, preds)
precision = precision_score(true, preds, average='weighted', zero_division=0)
recall = recall_score(true, preds, average='weighted', zero_division=0)
f1 = f1_score(true, preds, average='weighted', zero_division=0)

print("\nBioBERT Zero-Shot Learning Results:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision (weighted): {precision:.4f}")
print(f"Recall (weighted): {recall:.4f}")
print(f"F1 Score (weighted): {f1:.4f}")

"""##FSL

###DS1
"""

from transformers import AutoTokenizer, AutoModel
from datasets import load_dataset
import torch
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import random
import pandas as pd
from tqdm import tqdm

# Load dataset
ds = load_dataset("fhai50032/Symptoms_to_disease_7k")
df = pd.DataFrame(ds['train'])

# Extract symptom texts and disease labels
df['symptoms'] = df['query'].apply(lambda x: x.split('Patient:')[1].strip())
df['disease'] = df['response'].apply(lambda x: x.replace("You may have ", "").replace(".", ""))

# Load BioBERT model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
model = AutoModel.from_pretrained("dmis-lab/biobert-v1.1")

# Function to extract embeddings
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    # Use CLS token embedding as the sentence representation
    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
    return embedding

# Create few-shot training set
n_shots_per_class = 5  # Number of examples per disease
unique_diseases = df['disease'].unique()
print(f"Number of unique diseases: {len(unique_diseases)}")

# Sample few-shot examples
few_shot_samples = []
for disease in unique_diseases:
    # Get all examples of this disease
    disease_examples = df[df['disease'] == disease]
    # Sample n_shots_per_class examples (or all if fewer)
    n_samples = min(n_shots_per_class, len(disease_examples))
    sampled_examples = disease_examples.sample(n_samples)
    few_shot_samples.append(sampled_examples)

# Combine into few-shot training set
train_df = pd.concat(few_shot_samples).reset_index(drop=True)
print(f"Few-shot training set size: {len(train_df)}")

# Create test set from remaining examples
test_df = df[~df.index.isin(train_df.index)].sample(500)  # Sample 500 test examples
print(f"Test set size: {len(test_df)}")

# Generate embeddings for training data
print("Generating embeddings for training data...")
train_embeddings = np.array([get_embedding(text) for text in tqdm(train_df['symptoms'])])
train_labels = train_df['disease'].values

# Train KNN classifier
print("Training KNN classifier...")
knn = KNeighborsClassifier(n_neighbors=1)  # 1-NN for few-shot
knn.fit(train_embeddings, train_labels)

# Evaluate on test set
print("Evaluating on test set...")
correct = 0
total = len(test_df)

for idx, row in tqdm(test_df.iterrows(), total=total):
    symptoms = row['symptoms']
    true_disease = row['disease']

    # Get embedding for this example
    embedding = get_embedding(symptoms)

    # Predict disease
    predicted_disease = knn.predict([embedding])[0]

    if predicted_disease == true_disease:
        correct += 1

accuracy = correct / total
print(f"Few-shot accuracy: {accuracy:.4f}")
print("Evaluating on test set...")

y_true = []
y_pred = []

for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):
    symptoms = row['symptoms']
    true_disease = row['disease']

    # Get embedding for this example
    embedding = get_embedding(symptoms)

    # Predict disease
    predicted_disease = knn.predict([embedding])[0]

    y_true.append(true_disease)
    y_pred.append(predicted_disease)

# Calculate metrics
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')  # 'macro' for treating all classes equally

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")

"""###DS2"""

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModel
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import pandas as pd
import numpy as np
import torch
import random
import re
from tqdm import tqdm

# Set random seed for reproducibility
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)

# Load Dataset
print("Loading dataset...")
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

# Filter examples where instruction mentions symptoms
def is_symptom_query(text):
    return isinstance(text, str) and "I am having" in text

filtered_df = df[df['instruction'].apply(is_symptom_query)].copy()

# Extract symptoms and disease
def extract_symptoms(text):
    match = re.search(r"I am having the following symptoms:(.*)\. What could be the disease", text)
    if match:
        return match.group(1).strip()
    else:
        return None

def extract_disease(text):
    match = re.search(r"patient is dealing with (.*)\.", text)
    if match:
        return match.group(1).strip()
    else:
        return None

filtered_df['symptoms'] = filtered_df['instruction'].apply(extract_symptoms)
filtered_df['disease'] = filtered_df['output'].apply(extract_disease)

# Drop invalid rows
filtered_df = filtered_df.dropna(subset=['symptoms', 'disease']).reset_index(drop=True)
print(f"Filtered dataset size: {len(filtered_df)}")

# Load BioBERT model and tokenizer
print("Loading BioBERT model...")
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
model = AutoModel.from_pretrained("dmis-lab/biobert-v1.1")

# Function to extract sentence embeddings
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]  # CLS token
    return embedding

# Few-shot sampling
n_shots_per_class = 5
unique_diseases = filtered_df['disease'].unique()
print(f"Number of unique diseases: {len(unique_diseases)}")

few_shot_samples = []
for disease in unique_diseases:
    disease_examples = filtered_df[filtered_df['disease'] == disease]
    n_samples = min(n_shots_per_class, len(disease_examples))
    sampled_examples = disease_examples.sample(n_samples, random_state=42)
    few_shot_samples.append(sampled_examples)

train_df = pd.concat(few_shot_samples).reset_index(drop=True)
print(f"Few-shot training set size: {len(train_df)}")

# Create test set from remaining examples
test_df = filtered_df[~filtered_df.index.isin(train_df.index)].sample(500, random_state=42)
print(f"Test set size: {len(test_df)}")

# Generate embeddings for training data
print("Generating embeddings for training data...")
train_embeddings = np.array([get_embedding(text) for text in tqdm(train_df['symptoms'])])
train_labels = train_df['disease'].values

# Train KNN classifier
print("Training KNN classifier...")
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(train_embeddings, train_labels)

# Evaluate on test set
print("Evaluating on test set...")
y_true = []
y_pred = []

for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):
    symptoms = row['symptoms']
    true_disease = row['disease']
    embedding = get_embedding(symptoms)
    predicted_disease = knn.predict([embedding])[0]

    y_true.append(true_disease)
    y_pred.append(predicted_disease)

# Calculate evaluation metrics
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')

print("\n===== Evaluation Results =====")
print(f"Accuracy : {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall   : {recall:.4f}")
print(f"F1-Score : {f1:.4f}")

"""###DS3"""

from datasets import load_dataset
import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tqdm import tqdm
import torch
import random

# Load Dataset
print("Loading dataset...")
ds = load_dataset("Mohamed-Ahmed161/Disease-Symptoms")
df = pd.DataFrame(ds['train'])

# Clean and Prepare
print(f"Original dataset size: {len(df)}")
df = df[['Disease_Name', 'Generated_Sentence_From_symptoms']]
df = df.dropna()
df = df.rename(columns={'Disease_Name': 'disease', 'Generated_Sentence_From_symptoms': 'symptoms'})
print(f"Dataset size after cleaning: {len(df)}")

# Load BioBERT model
print("Loading BioBERT model...")
tokenizer = AutoTokenizer.from_pretrained("dmis-lab/biobert-v1.1")
model = AutoModel.from_pretrained("dmis-lab/biobert-v1.1")

# Function to get embeddings
def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
    return embedding

# Create Few-shot Dataset
n_shots_per_class = 1  # Only 1 sample per disease
unique_diseases = df['disease'].unique()
print(f"Number of unique diseases: {len(unique_diseases)}")

# Select 300 diseases randomly
random.seed(42)
selected_diseases = random.sample(list(unique_diseases), 300)

few_shot_samples = []
for disease in selected_diseases:
    disease_examples = df[df['disease'] == disease]
    n_samples = min(n_shots_per_class, len(disease_examples))
    sampled_examples = disease_examples.sample(n_samples, random_state=42)
    few_shot_samples.append(sampled_examples)

train_df = pd.concat(few_shot_samples).reset_index(drop=True)
print(f"Few-shot training set size: {len(train_df)}")

# Create Test Set
test_df = df[~df['disease'].isin(selected_diseases)]
print(f"Test set size: {len(test_df)}")

# Generate embeddings for train
print("Generating embeddings for training data...")
train_embeddings = np.array([get_embedding(text) for text in tqdm(train_df['symptoms'])])
train_labels = train_df['disease'].values

# Train KNN Classifier
print("Training KNN classifier...")
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(train_embeddings, train_labels)

# Evaluate on test set
print("Evaluating on test set...")
test_embeddings = np.array([get_embedding(text) for text in tqdm(test_df['symptoms'])])
test_labels = test_df['disease'].values

predictions = knn.predict(test_embeddings)

# Calculate Metrics
accuracy = accuracy_score(test_labels, predictions)
precision = precision_score(test_labels, predictions, average='weighted', zero_division=0)
recall = recall_score(test_labels, predictions, average='weighted', zero_division=0)
f1 = f1_score(test_labels, predictions, average='weighted', zero_division=0)

print("\nResults:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision (weighted): {precision:.4f}")
print(f"Recall (weighted): {recall:.4f}")
print(f"F1 Score (weighted): {f1:.4f}")

"""###DS4"""

import pandas as pd
import re, json, ast
import numpy as np
from datasets import Dataset, load_dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)

# Load dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")
df = pd.DataFrame(ds["train"])

# Extract disease name from prompt
def extract_disease(prompt):
    match = re.search(r'Doctor-patient conversation about (.+)', prompt)
    return match.group(1).strip() if match else None

# Extract patient symptoms
def extract_symptoms(messages):
    try:
        if isinstance(messages, str):
            try:
                messages = json.loads(messages)
            except:
                try:
                    messages = ast.literal_eval(messages)
                except:
                    matches = re.findall(r"'role': 'patient', 'content': '(.+?)'", messages)
                    return " ".join(matches)
        return " ".join(m.get("content", "") for m in messages if m.get("role") == "patient").strip()
    except:
        return ""

# Apply extraction
df["disease"] = df["prompt"].apply(extract_disease)
df["symptoms"] = df["messages"].apply(extract_symptoms)
df = df.dropna(subset=["disease", "symptoms"])
df = df[df["symptoms"] != ""]

# Select few-shot subset (5 diseases √ó 20 examples)
N, K = 5, 20
top_diseases = df["disease"].value_counts().head(N).index.tolist()
fewshot_df = df[df["disease"].isin(top_diseases)].groupby("disease").apply(
    lambda x: x.sample(n=min(K, len(x)), random_state=42)
).reset_index(drop=True)

# Train/test split
train_df, test_df = train_test_split(fewshot_df, test_size=0.2, random_state=42)
train_dataset = Dataset.from_pandas(train_df[["symptoms", "disease"]])
test_dataset = Dataset.from_pandas(test_df[["symptoms", "disease"]])

# Load BioBERT tokenizer
model_name = "dmis-lab/biobert-base-cased-v1.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Tokenize function
def preprocess(example):
    return tokenizer(example["symptoms"], truncation=True, padding="max_length", max_length=256)

train_dataset = train_dataset.map(preprocess, batched=True)
test_dataset = test_dataset.map(preprocess, batched=True)

# Encode labels
label_list = train_dataset.unique("disease")
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {v: k for k, v in label2id.items()}
train_dataset = train_dataset.map(lambda x: {"label": label2id[x["disease"]]})
test_dataset = test_dataset.map(lambda x: {"label": label2id[x["disease"]]})

# Load BioBERT model for classification
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))

# Training setup
training_args = TrainingArguments(
    output_dir="./biobert-fsl",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    learning_rate=2e-5,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
)

# Metric function
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="macro")
    }

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# Train BioBERT on few-shot data
trainer.train()

# Evaluate
results = trainer.evaluate()
print("‚úÖ BioBERT Few-Shot Evaluation:", results)

"""#BART

##imports
"""

pip install transformers datasets torch pandas

!pip install datasets

"""##ZSL

### DS1
"""

!pip install sacremoses  # Install required dependency for BioGPT tokenizer

import pandas as pd
import numpy as np
from datasets import load_dataset
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import torch

# Load dataset from Hugging Face
ds = load_dataset("fhai50032/Symptoms_to_disease_7k")
df = pd.DataFrame(ds['train'])

# Extract symptoms and diseases
df["symptoms"] = df["query"].str.extract(r'Patient:I may have (.*)')
df["disease"] = df["response"].str.replace("You may have ", "", regex=False)

# Drop missing values
df = df.dropna(subset=["symptoms", "disease"])

# Split Dataset (80% Train, 20% Test for evaluation)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

print(f"Training set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")

# Load BioGPT model and tokenizer
model_name = "microsoft/biogpt"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Get unique disease labels for classification
disease_counts = df["disease"].value_counts()
top_diseases = disease_counts.head(20).index.tolist()
print(f"Using top {len(top_diseases)} diseases as labels: {top_diseases}")

# Function to create prompts for BioGPT
def create_prompt(symptoms, disease_labels):
    return f"""Based on these symptoms: {symptoms}
Which of these diseases is most likely?
Options: {', '.join(disease_labels)}
Answer with just the disease name:"""

# Function to process in smaller batches to avoid memory issues
def predict_diseases_batch(symptom_list, disease_labels, batch_size=8):
    all_predictions = []

    for i in range(0, len(symptom_list), batch_size):
        batch = symptom_list[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(symptom_list) + batch_size - 1)//batch_size}")

        # Create prompts for the batch
        prompts = [create_prompt(symptoms, disease_labels) for symptoms in batch]

        # Tokenize and generate
        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=20,
                temperature=0.1,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )

        # Decode and clean predictions
        batch_predictions = []
        for j in range(len(outputs)):
            text = tokenizer.decode(outputs[j], skip_special_tokens=True)
            # Extract just the answer part
            answer = text.replace(prompts[j], "").strip()
            # Take first line and clean
            answer = answer.split('\n')[0].strip().rstrip('.').strip()
            batch_predictions.append(answer)

        all_predictions.extend(batch_predictions)

    return all_predictions

# Create a smaller test set for demonstration (50 samples)
small_test_df = test_df.sample(50, random_state=42)

# Apply batch inference to the small test set
small_test_df["predicted_disease"] = predict_diseases_batch(
    small_test_df["symptoms"].tolist(),
    top_diseases
)

# Display sample results
print("\nSample predictions:")
results_df = small_test_df[["symptoms", "disease", "predicted_disease"]].head(10)
print(results_df)

# Calculate Accuracy
accuracy = (small_test_df["disease"] == small_test_df["predicted_disease"]).mean()
print(f"\nBioGPT Accuracy on sample: {accuracy:.2%}")

# Check if any predictions match even when not exact matches (partial matches)
small_test_df["partial_match"] = small_test_df.apply(
    lambda row: row["disease"].lower() in row["predicted_disease"].lower() or
               row["predicted_disease"].lower() in row["disease"].lower(),
    axis=1
)
partial_accuracy = small_test_df["partial_match"].mean()
print(f"Partial match accuracy: {partial_accuracy:.2%}")

# Generate a classification report for the diseases in our sample
print("\nClassification Report:")
# Filter to only diseases that appear in our small test set for the report
present_diseases = small_test_df["disease"].unique()
relevant_mask = small_test_df["disease"].isin(present_diseases)
report = classification_report(
    small_test_df.loc[relevant_mask, "disease"],
    small_test_df.loc[relevant_mask, "predicted_disease"],
    zero_division=0
)
print(report)

"""### DS2"""

import pandas as pd
from datasets import load_dataset

# Load dataset
ds = load_dataset("prognosis/symptoms_disease_v1")

# Convert to Pandas DataFrame (assuming it has a 'train' split)
df = pd.DataFrame(ds["train"])

# Display the first 5 rows
print(df.head())

import pandas as pd
import numpy as np
from datasets import load_dataset
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import re

# Load the dataset from Hugging Face
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

print(f"Original dataset size: {len(df)}")
print("Dataset columns:", df.columns.tolist())

# Create empty DataFrame to store processed data
processed_data = []

# Process each row to extract symptoms and diseases
for _, row in df.iterrows():
    instruction = row['instruction']
    output = row['output']

    # Case 1: Instruction asks about disease diagnosis from symptoms
    if "I am having the following symptoms:" in instruction and "What could be the disease" in instruction:
        # Extract symptoms from instruction
        symptoms_match = re.search(r'I am having the following symptoms: (.*?)(?:\.|$)', instruction)
        if symptoms_match:
            symptoms = symptoms_match.group(1).strip()

            # Extract disease from output
            disease_match = re.search(r'dealing with (.*?)(?:\.|$)', output)
            if disease_match:
                disease = disease_match.group(1).strip()
                processed_data.append({"symptoms": symptoms, "disease": disease})

    # We don't process the "What are the symptoms of X disease?" format
    # as it's not useful for our symptoms->disease prediction task

# Create processed DataFrame
df_processed = pd.DataFrame(processed_data)

print(f"Processed dataset size: {len(df_processed)}")
print("Sample processed data:")
print(df_processed.head(3))

# Check for most common diseases
print("\nMost common diseases:")
print(df_processed['disease'].value_counts().head(10))

# Split Dataset (80% Train, 20% Test for evaluation)
if len(df_processed) > 0:
    train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)

    print(f"Training set size: {len(train_df)}")
    print(f"Test set size: {len(test_df)}")

    # Load BART Zero-Shot Classifier
    classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

    # Get unique disease labels for classification
    # We'll use only the top 20 most common diseases to make processing faster
    disease_counts = df_processed["disease"].value_counts()
    top_diseases = disease_counts.head(20).index.tolist()
    print(f"Using top {len(top_diseases)} diseases as labels: {top_diseases}")

    # Function to process in smaller batches to avoid memory issues
    def predict_diseases_batch(symptom_list, disease_labels, batch_size=8):
        all_predictions = []

        for i in range(0, len(symptom_list), batch_size):
            batch = symptom_list[i:i+batch_size]
            print(f"Processing batch {i//batch_size + 1}/{(len(symptom_list) + batch_size - 1)//batch_size}")

            results = classifier(batch, disease_labels, multi_label=False)
            batch_predictions = [res["labels"][0] for res in results]  # Get the top prediction for each
            all_predictions.extend(batch_predictions)

        return all_predictions

    # Create a smaller test set for demonstration (50 samples)
    small_test_df = test_df.sample(min(50, len(test_df)), random_state=42)

    # Apply batch inference to the small test set
    small_test_df["predicted_disease"] = predict_diseases_batch(
        small_test_df["symptoms"].tolist(),
        top_diseases
    )

    # Display sample results
    print("\nSample predictions:")
    results_df = small_test_df[["symptoms", "disease", "predicted_disease"]].head(10)
    print(results_df)

    # Calculate Accuracy
    accuracy = (small_test_df["disease"] == small_test_df["predicted_disease"]).mean()
    print(f"\nZero-Shot BART Accuracy on sample: {accuracy:.2%}")

    # Check if any predictions match even when not exact matches (partial matches)
    small_test_df["partial_match"] = small_test_df.apply(
        lambda row: row["disease"] in row["predicted_disease"] or row["predicted_disease"] in row["disease"],
        axis=1
    )
    partial_accuracy = small_test_df["partial_match"].mean()
    print(f"Partial match accuracy: {partial_accuracy:.2%}")

    # Generate a classification report for the diseases in our sample
    print("\nClassification Report:")
    # Filter to only diseases that appear in our small test set for the report
    present_diseases = small_test_df["disease"].unique()
    relevant_mask = small_test_df["disease"].isin(present_diseases)
    report = classification_report(
        small_test_df.loc[relevant_mask, "disease"],
        small_test_df.loc[relevant_mask, "predicted_disease"],
        zero_division=0
    )
    print(report)
else:
    print("No matching data found. Check the extraction patterns.")

"""### DS3"""

import pandas as pd
from datasets import load_dataset

# Load dataset
ds = load_dataset("Mohamed-Ahmed161/Disease-Symptoms")

# Convert to Pandas DataFrame (assuming it has a 'train' split)
df = pd.DataFrame(ds["train"])

# Display the first 5 rows
print(df.head())

import pandas as pd
import ast
from datasets import load_dataset
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Load dataset
ds = load_dataset("Mohamed-Ahmed161/Disease-Symptoms")
df = pd.DataFrame(ds['train'])

print(f"Original dataset size: {len(df)}")

# Process Symptom_json column efficiently
def extract_symptoms(symptom_json_str):
    try:
        # Parse the JSON string
        symptom_dict = ast.literal_eval(symptom_json_str)
        symptoms = symptom_dict.get('symptoms', [])
        return " | ".join(symptoms) if symptoms else ""
    except:
        return ""

# Extract symptoms text in one go
df['symptoms_text'] = df['Symptom_json'].apply(extract_symptoms)
df = df[df['symptoms_text'] != ""]

print(f"Processed dataset size: {len(df)}")
print("Sample processed data:")
print(df[['Disease_Name', 'symptoms_text']].head(3))

# Check most common diseases
print("\nMost common diseases:")
print(df['Disease_Name'].value_counts().head(10))

# Split Dataset
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

print(f"Training set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")

# Load BART Zero-Shot Classifier
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Get only top 20 disease labels to improve efficiency
disease_counts = df["Disease_Name"].value_counts()
top_diseases = disease_counts.head(20).index.tolist()
print(f"Using top {len(top_diseases)} diseases as labels: {top_diseases}")

# Sample a small test set for quick demonstration
sample_size = min(50, len(test_df))
small_test_df = test_df.sample(sample_size, random_state=42)
print(f"Using small test sample of {sample_size} examples")

# Batch processing function (more efficient)
def predict_diseases_batch(symptom_list, disease_labels, batch_size=8):
    all_predictions = []

    for i in range(0, len(symptom_list), batch_size):
        batch = symptom_list[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(symptom_list) + batch_size - 1)//batch_size}")

        # Process the entire batch at once
        results = classifier(batch, disease_labels, multi_label=False)
        batch_predictions = [res["labels"][0] for res in results]
        all_predictions.extend(batch_predictions)

    return all_predictions

# Run predictions on the small test set
small_test_df["predicted_disease"] = predict_diseases_batch(
    small_test_df["symptoms_text"].tolist(),
    top_diseases
)

# Display sample results
print("\nSample predictions:")
results_df = small_test_df[["symptoms_text", "Disease_Name", "predicted_disease"]].head(10)
print(results_df)

# Calculate Accuracy
accuracy = (small_test_df["Disease_Name"] == small_test_df["predicted_disease"]).mean()
print(f"\nZero-Shot BART Accuracy on sample: {accuracy:.2%}")

# Check partial matches
small_test_df["partial_match"] = small_test_df.apply(
    lambda row: row["Disease_Name"] in row["predicted_disease"] or row["predicted_disease"] in row["Disease_Name"],
    axis=1
)
partial_accuracy = small_test_df["partial_match"].mean()
print(f"Partial match accuracy: {partial_accuracy:.2%}")

# Generate classification report
print("\nClassification Report:")
report = classification_report(
    small_test_df["Disease_Name"],
    small_test_df["predicted_disease"],
    zero_division=0
)
print(report)
"""
# Save results to CSV
small_test_df[["symptoms_text", "Disease_Name", "predicted_disease"]].to_csv(
    "disease_prediction_results_sample.csv", index=False
)
print("\nSample results saved to disease_prediction_results_sample.csv")"""

"""### DS4"""

import pandas as pd
from datasets import load_dataset

# Load dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")


# Convert to Pandas DataFrame (assuming it has a 'train' split)
df = pd.DataFrame(ds["train"])

# Display the first 5 rows
print(df.head())

import pandas as pd
import re
import json
import ast
from datasets import load_dataset
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Load dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")
df = pd.DataFrame(ds['train'])

print(f"Original dataset size: {len(df)}")
print("Dataset columns:", df.columns.tolist())

# Print sample of messages column to understand the structure
print("\nSample messages structure:")
print(type(df['messages'].iloc[0]))
print(df['messages'].iloc[0][:500])  # Print first 500 chars

# Extract disease names from prompt column
def extract_disease(prompt):
    match = re.search(r'Doctor-patient conversation about (.+)', prompt)
    if match:
        return match.group(1).strip()
    return None

# Extract patient symptoms from messages
def extract_symptoms(messages):
    try:
        # Try parsing as JSON string first
        if isinstance(messages, str):
            try:
                messages_list = json.loads(messages)
            except:
                try:
                    messages_list = ast.literal_eval(messages)
                except:
                    # If both fail, try parsing with regex
                    symptoms = []
                    pattern = r"'role': 'patient', 'content': '(.+?)'"
                    matches = re.findall(pattern, messages)
                    return " ".join(matches)
        else:
            messages_list = messages

        # Now extract symptoms from parsed messages
        symptoms_text = ""
        for message in messages_list:
            if isinstance(message, dict) and message.get('role') == 'patient':
                symptoms_text += message.get('content', '') + " "
        return symptoms_text.strip()
    except Exception as e:
        print(f"Error processing message: {e}")
        return ""

# Process the dataset
df['disease'] = df['prompt'].apply(extract_disease)

# Try to extract a sample first to debug
print("\nTesting symptom extraction on first example...")
sample_symptoms = extract_symptoms(df['messages'].iloc[0])
print(f"Extracted symptoms: {sample_symptoms[:200]}...")

# Now apply to all rows
df['symptoms'] = df['messages'].apply(extract_symptoms)

# Remove rows with missing data
df = df.dropna(subset=['disease', 'symptoms'])
df = df[df['symptoms'] != ""]  # Remove rows with empty symptoms
print(f"Processed dataset size: {len(df)}")

# Display sample data
print("\nSample processed data:")
for i, row in df[['disease', 'symptoms']].head(2).iterrows():
    print(f"\nDisease: {row['disease']}")
    print(f"Symptoms: {row['symptoms'][:200]}...")

# Check most common diseases
print("\nMost common diseases:")
print(df['disease'].value_counts().head(10))

# Split Dataset
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

print(f"Training set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")

# Load BART Zero-Shot Classifier
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Get top 20 disease labels to improve efficiency
disease_counts = df['disease'].value_counts()
top_diseases = disease_counts.head(20).index.tolist()
print(f"Using top {len(top_diseases)} diseases as labels: {top_diseases}")

# Sample a small test set for quick demonstration
sample_size = min(50, len(test_df))
small_test_df = test_df.sample(sample_size, random_state=42)
print(f"Using small test sample of {sample_size} examples")

# Batch processing function
def predict_diseases_batch(symptom_list, disease_labels, batch_size=8):
    all_predictions = []

    for i in range(0, len(symptom_list), batch_size):
        batch = symptom_list[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(symptom_list) + batch_size - 1)//batch_size}")

        # Process the entire batch at once
        results = classifier(batch, disease_labels, multi_label=False)
        batch_predictions = [res["labels"][0] for res in results]
        all_predictions.extend(batch_predictions)

    return all_predictions

# Run predictions on the small test set
small_test_df["predicted_disease"] = predict_diseases_batch(
    small_test_df["symptoms"].tolist(),
    top_diseases
)

# Display sample results
print("\nSample predictions:")
for i, row in small_test_df[["symptoms", "disease", "predicted_disease"]].head(5).iterrows():
    print(f"\nActual: {row['disease']}")
    print(f"Predicted: {row['predicted_disease']}")
    print(f"Symptoms: {row['symptoms'][:200]}...")  # Show first 200 chars of symptoms

# Calculate Accuracy
accuracy = (small_test_df["disease"] == small_test_df["predicted_disease"]).mean()
print(f"\nZero-Shot BART Accuracy on sample: {accuracy:.2%}")

# Check partial matches
small_test_df["partial_match"] = small_test_df.apply(
    lambda row: row["disease"] in row["predicted_disease"] or row["predicted_disease"] in row["disease"],
    axis=1
)
partial_accuracy = small_test_df["partial_match"].mean()
print(f"Partial match accuracy: {partial_accuracy:.2%}")

# Generate classification report
print("\nClassification Report:")
report = classification_report(
    small_test_df["disease"],
    small_test_df["predicted_disease"],
    zero_division=0
)
print(report)

# Save results to CSV
small_test_df[["symptoms", "disease", "predicted_disease", "partial_match"]].to_csv(
    "chat_disease_prediction_results.csv", index=False
)
print("\nSample results saved to chat_disease_prediction_results.csv")

"""##FSL

### DS1
"""

# Install required packages (run only once)
!pip install datasets transformers

# ‚úÖ Imports
import pandas as pd
import torch  # <-- this is the missing line üí°
from datasets import load_dataset
from transformers import pipeline
import random

# ‚úÖ Load dataset
dataset = load_dataset("fhai50032/Symptoms_to_disease_7k")["train"]

# ‚úÖ Convert to DataFrame
df = pd.DataFrame(dataset)

# ‚úÖ Clean and extract relevant text
df["symptoms"] = df["query"].str.extract(r'Patient:I may have (.*)')
df["disease"] = df["response"].str.replace("You may have ", "", regex=False)

# ‚úÖ Drop NA values
df.dropna(subset=["symptoms", "disease"], inplace=True)

# ‚úÖ Few-shot training: 10 examples per disease
few_shot_df = df.groupby("disease").apply(lambda x: x.sample(min(len(x), 10), random_state=42)).reset_index(drop=True)

# ‚úÖ Test set: 100 random samples not used in training
test_df = df.drop(few_shot_df.index).sample(100, random_state=42)

# ‚úÖ Get label list from few-shot training data
label_set = few_shot_df["disease"].unique().tolist()

# ‚úÖ Load BART Zero-Shot Classifier
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli", device=0 if torch.cuda.is_available() else -1)

# ‚úÖ Predict function
def predict_disease(symptoms):
    result = classifier(symptoms, label_set, multi_label=False)
    return result["labels"][0]

# ‚úÖ Make predictions
test_df["predicted"] = test_df["symptoms"].apply(predict_disease)

# ‚úÖ Calculate and print accuracy
accuracy = (test_df["disease"] == test_df["predicted"]).mean()
print(f"\nüéØ Few-Shot Accuracy with BART: {accuracy:.2%}")

# ‚úÖ Show sample predictions
print(test_df[["symptoms", "disease", "predicted"]].head(10))

"""###DS2

"""

import pandas as pd
import numpy as np
from datasets import load_dataset
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import re
import random

# Load the dataset from Hugging Face
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

print(f"Original dataset size: {len(df)}")
print("Dataset columns:", df.columns.tolist())

# Create empty DataFrame to store processed data
processed_data = []

# Process each row to extract symptoms and diseases
for _, row in df.iterrows():
    instruction = row['instruction']
    output = row['output']

    # Case 1: Instruction asks about disease diagnosis from symptoms
    if "I am having the following symptoms:" in instruction and "What could be the disease" in instruction:
        # Extract symptoms from instruction
        symptoms_match = re.search(r'I am having the following symptoms: (.*?)(?:\.|$)', instruction)
        if symptoms_match:
            symptoms = symptoms_match.group(1).strip()

            # Extract disease from output
            disease_match = re.search(r'dealing with (.*?)(?:\.|$)', output)
            if disease_match:
                disease = disease_match.group(1).strip()
                processed_data.append({"symptoms": symptoms, "disease": disease})

# Create processed DataFrame
df_processed = pd.DataFrame(processed_data)

print(f"Processed dataset size: {len(df_processed)}")
print("Sample processed data:")
print(df_processed.head(3))

# Check for most common diseases
print("\nMost common diseases:")
print(df_processed['disease'].value_counts().head(10))

# Split Dataset (80% Train, 20% Test for evaluation)
if len(df_processed) > 0:
    train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)

    print(f"Training set size: {len(train_df)}")
    print(f"Test set size: {len(test_df)}")

    # Load BART model for text generation
    generator = pipeline("text-generation", model="facebook/bart-large")

    # Get unique disease labels for classification
    disease_counts = df_processed["disease"].value_counts()
    top_diseases = disease_counts.head(20).index.tolist()
    print(f"Using top {len(top_diseases)} diseases as labels: {top_diseases}")

    # Function to create few-shot prompt
    def create_few_shot_prompt(symptoms, train_samples, n_shots=3):
        prompt = "Diagnose the disease based on symptoms. Examples:\n\n"

        # Select random few-shot examples
        few_shot_examples = train_samples.sample(n_shots)

        for _, example in few_shot_examples.iterrows():
            prompt += f"Symptoms: {example['symptoms']}\nDisease: {example['disease']}\n\n"

        prompt += f"Symptoms: {symptoms}\nDisease:"
        return prompt

    # Function to process in smaller batches to avoid memory issues
    def predict_diseases_few_shot(test_samples, train_samples, batch_size=4, n_shots=3):
        all_predictions = []

        for i in range(0, len(test_samples), batch_size):
            batch = test_samples.iloc[i:i+batch_size]
            print(f"Processing batch {i//batch_size + 1}/{(len(test_samples) + batch_size - 1)//batch_size}")

            batch_predictions = []
            for _, row in batch.iterrows():
                prompt = create_few_shot_prompt(row['symptoms'], train_samples, n_shots)

                # Generate prediction
                result = generator(
                    prompt,
                    max_length=len(prompt.split()) + 10,  # Limit output length
                    num_return_sequences=1,
                    do_sample=False
                )

                # Extract the generated text
                generated_text = result[0]['generated_text']

                # Extract the predicted disease (text after "Disease:")
                prediction = generated_text.split("Disease:")[-1].strip()

                # Clean up prediction - take first line and remove any punctuation
                prediction = prediction.split('\n')[0].split('.')[0].strip()

                # Find the closest matching disease from our labels
                closest_disease = None
                max_similarity = 0
                for disease in top_diseases:
                    # Simple similarity check (could be improved)
                    similarity = sum(1 for word in disease.lower().split()
                                   if word in prediction.lower())
                    if similarity > max_similarity:
                        max_similarity = similarity
                        closest_disease = disease

                batch_predictions.append(closest_disease or "Unknown")

            all_predictions.extend(batch_predictions)

        return all_predictions

    # Create a smaller test set for demonstration (20 samples)
    small_test_df = test_df.sample(min(20, len(test_df)), random_state=42)

    # Apply few-shot inference to the small test set
    small_test_df["predicted_disease"] = predict_diseases_few_shot(
        small_test_df,
        train_df,
        batch_size=4,
        n_shots=3  # Number of few-shot examples
    )

    # Display sample results
    print("\nSample predictions:")
    results_df = small_test_df[["symptoms", "disease", "predicted_disease"]].head(10)
    print(results_df)

    # Calculate Accuracy
    accuracy = (small_test_df["disease"] == small_test_df["predicted_disease"]).mean()
    print(f"\nFew-Shot BART Accuracy on sample: {accuracy:.2%}")

    # Check if any predictions match even when not exact matches (partial matches)
    small_test_df["partial_match"] = small_test_df.apply(
        lambda row: row["disease"] in row["predicted_disease"] or row["predicted_disease"] in row["disease"],
        axis=1
    )
    partial_accuracy = small_test_df["partial_match"].mean()
    print(f"Partial match accuracy: {partial_accuracy:.2%}")

    # Generate a classification report for the diseases in our sample
    print("\nClassification Report:")
    # Filter to only diseases that appear in our small test set for the report
    present_diseases = small_test_df["disease"].unique()
    relevant_mask = small_test_df["disease"].isin(present_diseases)
    report = classification_report(
        small_test_df.loc[relevant_mask, "disease"],
        small_test_df.loc[relevant_mask, "predicted_disease"],
        zero_division=0
    )
    print(report)
else:
    print("No matching data found. Check the extraction patterns.")

"""###DS3"""

import pandas as pd
import ast
import torch
from transformers import pipeline, BartTokenizer, BartForConditionalGeneration
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import random

# Load dataset
df = pd.read_parquet("hf://datasets/Mohamed-Ahmed161/Disease-Symptoms/data/train-00000-of-00001.parquet")

print(f"Original dataset size: {len(df)}")

# Process Symptom_json column efficiently
def extract_symptoms(symptom_json_str):
    try:
        symptom_dict = ast.literal_eval(symptom_json_str)
        symptoms = symptom_dict.get('symptoms', [])
        return ", ".join(symptoms) if symptoms else ""
    except:
        return ""

df['symptoms_text'] = df['Symptom_json'].apply(extract_symptoms)
df = df[df['symptoms_text'] != ""]

print(f"Processed dataset size: {len(df)}")
print("Sample processed data:")
print(df[['Disease_Name', 'symptoms_text']].head(3))

# Check most common diseases
print("\nMost common diseases:")
print(df['Disease_Name'].value_counts().head(10))

# Split Dataset
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

print(f"Training set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")

# Load BART model and tokenizer for few-shot learning
device = 0 if torch.cuda.is_available() else -1
model_name = "facebook/bart-large"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name).to(device)

# Get only top 20 disease labels to improve efficiency
disease_counts = df["Disease_Name"].value_counts()
top_diseases = disease_counts.head(20).index.tolist()
print(f"Using top {len(top_diseases)} diseases as labels: {top_diseases}")

# Sample a small test set for quick demonstration
sample_size = min(50, len(test_df))
small_test_df = test_df.sample(sample_size, random_state=42)
print(f"Using small test sample of {sample_size} examples")

def create_few_shot_prompt(symptoms, examples, n_shots=3):
    """Create prompt with n-shot examples"""
    prompt = "Diagnose the disease based on symptoms. Examples:\n\n"

    for _, example in examples.iterrows():
        prompt += f"Symptoms: {example['symptoms_text']}\nDiagnosis: {example['Disease_Name']}\n\n"

    prompt += f"Symptoms: {symptoms}\nDiagnosis:"
    return prompt

def predict_with_bart(prompt, max_length=50):
    """Generate prediction using BART"""
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024).to(device)
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_beams=5,
        early_stopping=True,
        no_repeat_ngram_size=2
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def predict_diseases_few_shot(test_samples, train_samples, n_shots=3, batch_size=4):
    """Batch prediction with few-shot learning"""
    all_predictions = []
    total_batches = (len(test_samples) + batch_size - 1) // batch_size

    for i in range(0, len(test_samples), batch_size):
        batch = test_samples.iloc[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{total_batches}")

        batch_predictions = []
        for _, row in batch.iterrows():
            # Get random few-shot examples
            few_shot_examples = train_samples.sample(n_shots)
            prompt = create_few_shot_prompt(row['symptoms_text'], few_shot_examples, n_shots)

            # Generate prediction
            generated_text = predict_with_bart(prompt)

            # Extract the diagnosis (text after "Diagnosis:")
            prediction = generated_text.split("Diagnosis:")[-1].strip()

            # Clean up prediction
            prediction = prediction.split('\n')[0].split('.')[0].strip()

            # Find closest matching disease from our labels
            closest_disease = None
            max_similarity = 0
            for disease in top_diseases:
                similarity = sum(1 for word in disease.lower().split()
                               if word in prediction.lower())
                if similarity > max_similarity:
                    max_similarity = similarity
                    closest_disease = disease

            batch_predictions.append(closest_disease or "Unknown")

        all_predictions.extend(batch_predictions)

    return all_predictions

# Run few-shot predictions
small_test_df["predicted_disease"] = predict_diseases_few_shot(
    small_test_df,
    train_df,
    n_shots=3,
    batch_size=4
)

# Display results
print("\nSample predictions:")
results_df = small_test_df[["symptoms_text", "Disease_Name", "predicted_disease"]].head(10)
print(results_df.to_string(index=False, justify='left'))

# Calculate Accuracy
accuracy = (small_test_df["Disease_Name"] == small_test_df["predicted_disease"]).mean()
print(f"\nFew-Shot BART Accuracy on sample: {accuracy:.2%}")

# Enhanced partial matching
small_test_df["partial_match"] = small_test_df.apply(
    lambda row: (row["Disease_Name"].lower() in row["predicted_disease"].lower()) or
               (row["predicted_disease"].lower() in row["Disease_Name"].lower()) or
               any(word in row["predicted_disease"].lower().split()
                   for word in row["Disease_Name"].lower().split()),
    axis=1
)
partial_accuracy = small_test_df["partial_match"].mean()
print(f"Enhanced partial match accuracy: {partial_accuracy:.2%}")

# Classification Report
print("\nClassification Report:")
report = classification_report(
    small_test_df["Disease_Name"],
    small_test_df["predicted_disease"],
    zero_division=0,
    output_dict=False
)
print(report)

# Save results
output_cols = ["symptoms_text", "Disease_Name", "predicted_disease", "partial_match"]
small_test_df[output_cols].to_csv(
    "bart_fsl_disease_predictions.csv",
    index=False
)
print("\nResults saved to bart_fsl_disease_predictions.csv")

"""###DS4"""

import pandas as pd
import numpy as np
import re
import json
import ast
from datasets import load_dataset, Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)

# Load dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")
df = pd.DataFrame(ds['train'])

# Extract disease names from prompt
def extract_disease(prompt):
    match = re.search(r'Doctor-patient conversation about (.+)', prompt)
    return match.group(1).strip() if match else None

# Extract patient symptoms
def extract_symptoms(messages):
    try:
        if isinstance(messages, str):
            try:
                messages = json.loads(messages)
            except:
                try:
                    messages = ast.literal_eval(messages)
                except:
                    matches = re.findall(r"'role': 'patient', 'content': '(.+?)'", messages)
                    return " ".join(matches)
        symptoms = " ".join(msg.get("content", "") for msg in messages if msg.get("role") == "patient")
        return symptoms.strip()
    except:
        return ""

# Process data
df['disease'] = df['prompt'].apply(extract_disease)
df['symptoms'] = df['messages'].apply(extract_symptoms)
df = df.dropna(subset=['disease', 'symptoms'])
df = df[df['symptoms'] != ""]

# Few-shot subset: N classes, K examples each
N, K = 5, 20
top_diseases = df['disease'].value_counts().head(N).index.tolist()
fewshot_df = df[df['disease'].isin(top_diseases)].groupby('disease').apply(lambda x: x.sample(n=min(K, len(x)), random_state=42)).reset_index(drop=True)

# Train/test split
train_df, test_df = train_test_split(fewshot_df, test_size=0.2, random_state=42)
train_dataset = Dataset.from_pandas(train_df[['symptoms', 'disease']])
test_dataset = Dataset.from_pandas(test_df[['symptoms', 'disease']])

# Tokenization
tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")
def preprocess(example):
    return tokenizer(example["symptoms"], truncation=True, padding="max_length", max_length=256)
train_dataset = train_dataset.map(preprocess, batched=True)
test_dataset = test_dataset.map(preprocess, batched=True)

# Label encoding
label_list = train_dataset.unique('disease')
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {v: k for k, v in label2id.items()}
train_dataset = train_dataset.map(lambda e: {"label": label2id[e["disease"]]})
test_dataset = test_dataset.map(lambda e: {"label": label2id[e["disease"]]})

# Model setup
model = AutoModelForSequenceClassification.from_pretrained("facebook/bart-base", num_labels=len(label2id))
training_args = TrainingArguments(
    output_dir="./bart-fsl",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    learning_rate=2e-5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
)
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average="macro"),
    }

# Training
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
trainer.train()

# Evaluation
results = trainer.evaluate()
print("Few-Shot Evaluation:", results)

"""#Bio_ClinicalBERT

##ZSL

###DS1
"""

import pandas as pd
from datasets import load_dataset
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import torch

# Load dataset from Hugging Face
ds = load_dataset("fhai50032/Symptoms_to_disease_7k")
df = pd.DataFrame(ds['train'])

# Extract symptoms and diseases
df["symptoms"] = df["query"].str.extract(r'Patient:I may have (.*)')
df["disease"] = df["response"].str.replace("You may have ", "", regex=False)

# Drop missing values
df = df.dropna(subset=["symptoms", "disease"])

# Split Dataset (80% Train, 20% Test for evaluation)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

print(f"Training set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")

# Load Bio_ClinicalBERT Zero-Shot Classifier
device = 0 if torch.cuda.is_available() else -1
classifier = pipeline(
    "zero-shot-classification",
    model="emilyalsentzer/Bio_ClinicalBERT",
    device=device
)

# Get unique disease labels for classification
# Using top 20 most common diseases for faster processing
disease_counts = df["disease"].value_counts()
top_diseases = disease_counts.head(20).index.tolist()
print(f"Using top {len(top_diseases)} diseases as labels: {top_diseases}")

# Function to process in smaller batches with progress tracking
def predict_diseases_batch(symptom_list, disease_labels, batch_size=8):
    all_predictions = []
    total_batches = (len(symptom_list) + batch_size - 1) // batch_size

    for i in range(0, len(symptom_list), batch_size):
        batch = symptom_list[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{total_batches}")

        results = classifier(batch, disease_labels, multi_label=False)
        batch_predictions = [res["labels"][0] for res in results]
        all_predictions.extend(batch_predictions)

    return all_predictions

# Create a smaller test set for demonstration (50 samples)
small_test_df = test_df.sample(50, random_state=42)

# Apply batch inference to the small test set
small_test_df["predicted_disease"] = predict_diseases_batch(
    small_test_df["symptoms"].tolist(),
    top_diseases
)

# Display sample results
print("\nSample predictions:")
results_df = small_test_df[["symptoms", "disease", "predicted_disease"]].head(10)
print(results_df.to_string(index=False))

# Calculate Accuracy
accuracy = (small_test_df["disease"] == small_test_df["predicted_disease"]).mean()
print(f"\nZero-Shot Bio_ClinicalBERT Accuracy on sample: {accuracy:.2%}")

# Check for partial matches (useful for similar disease names)
small_test_df["partial_match"] = small_test_df.apply(
    lambda row: row["disease"].lower() in row["predicted_disease"].lower() or
               row["predicted_disease"].lower() in row["disease"].lower(),
    axis=1
)
partial_accuracy = small_test_df["partial_match"].mean()
print(f"Partial match accuracy: {partial_accuracy:.2%}")

# Generate classification report for diseases in our sample
print("\nClassification Report:")
present_diseases = small_test_df["disease"].unique()
relevant_mask = small_test_df["disease"].isin(present_diseases)
report = classification_report(
    small_test_df.loc[relevant_mask, "disease"],
    small_test_df.loc[relevant_mask, "predicted_disease"],
    zero_division=0
)
print(report)

"""###DS2"""

import pandas as pd
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import re
import torch

# Load dataset
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

print(f"Dataset size: {len(df)}")
print("Dataset columns:", df.columns.tolist())

# Process data to extract symptoms and diseases
processed_data = []
for _, row in df.iterrows():
    instruction = row['instruction']
    output = row['output']

    # Extract symptoms from instruction
    symptoms_match = re.search(r'I am having the following symptoms: (.*?)(?:\.|$)', instruction)
    if symptoms_match:
        symptoms = symptoms_match.group(1).strip()

        # Extract disease from output
        disease_match = re.search(r'dealing with (.*?)(?:\.|$)', output)
        if disease_match:
            disease = disease_match.group(1).strip()
            processed_data.append({"symptoms": symptoms, "disease": disease})

# Create new DataFrame with extracted data
df_processed = pd.DataFrame(processed_data)

print(f"\nProcessed dataset size: {len(df_processed)}")
print("Sample processed data:")
print(df_processed.head())

# Split into train/test
train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)

print(f"\nTraining set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")

# Get top diseases
top_diseases = df_processed['disease'].value_counts().head(20).index.tolist()
print(f"\nTop {len(top_diseases)} diseases:")
print(top_diseases)

# Initialize zero-shot classifier
device = 0 if torch.cuda.is_available() else -1
classifier = pipeline(
    "zero-shot-classification",
    model="emilyalsentzer/Bio_ClinicalBERT",
    device=device
)

# Batch prediction function
def predict_batch(texts, labels, batch_size=8):
    predictions = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        results = classifier(batch, labels, multi_label=False)
        predictions.extend([res['labels'][0] for res in results])
    return predictions

# Evaluate on small subset
test_sample = test_df.sample(50, random_state=42)
test_sample['predicted'] = predict_batch(
    test_sample['symptoms'].tolist(),
    top_diseases
)

# Calculate metrics
accuracy = (test_sample['disease'] == test_sample['predicted']).mean()
print(f"\nAccuracy: {accuracy:.2%}")

# Partial matches (case insensitive)
test_sample['partial_match'] = test_sample.apply(
    lambda x: x['disease'].lower() in x['predicted'].lower() or
             x['predicted'].lower() in x['disease'].lower(),
    axis=1
)
print(f"Partial match accuracy: {test_sample['partial_match'].mean():.2%}")

# Classification report
print("\nClassification Report:")
print(classification_report(
    test_sample['disease'],
    test_sample['predicted'],
    zero_division=0
))

# Example prediction
sample = "headache, fever, chills"
pred = classifier(sample, top_diseases)
print(f"\nExample prediction for '{sample}':")
for label, score in zip(pred['labels'][:3], pred['scores'][:3]):
    print(f"- {label}: {score:.2%}")

"""###DS3"""

import pandas as pd
import ast
import torch
from transformers import pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Load dataset
df = pd.read_parquet("hf://datasets/Mohamed-Ahmed161/Disease-Symptoms/data/train-00000-of-00001.parquet")

print(f"Original dataset size: {len(df)}")

# Process Symptom_json column efficiently
def extract_symptoms(symptom_json_str):
    try:
        # Parse the JSON string
        symptom_dict = ast.literal_eval(symptom_json_str)
        symptoms = symptom_dict.get('symptoms', [])
        return ", ".join(symptoms) if symptoms else ""
    except:
        return ""

# Extract symptoms text in one go
df['symptoms_text'] = df['Symptom_json'].apply(extract_symptoms)
df = df[df['symptoms_text'] != ""]

print(f"Processed dataset size: {len(df)}")
print("Sample processed data:")
print(df[['Disease_Name', 'symptoms_text']].head(3))

# Check most common diseases
print("\nMost common diseases:")
print(df['Disease_Name'].value_counts().head(10))

# Split Dataset
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

print(f"Training set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")

# Load Bio_ClinicalBERT Zero-Shot Classifier
device = 0 if torch.cuda.is_available() else -1
classifier = pipeline(
    "zero-shot-classification",
    model="emilyalsentzer/Bio_ClinicalBERT",
    device=device
)

# Get only top 20 disease labels to improve efficiency
disease_counts = df["Disease_Name"].value_counts()
top_diseases = disease_counts.head(20).index.tolist()
print(f"Using top {len(top_diseases)} diseases as labels: {top_diseases}")

# Sample a small test set for quick demonstration
sample_size = min(50, len(test_df))
small_test_df = test_df.sample(sample_size, random_state=42)
print(f"Using small test sample of {sample_size} examples")

# Batch processing function optimized for Bio_ClinicalBERT
def predict_diseases_batch(symptom_list, disease_labels, batch_size=8):
    all_predictions = []
    total_batches = (len(symptom_list) + batch_size - 1) // batch_size

    for i in range(0, len(symptom_list), batch_size):
        batch = symptom_list[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{total_batches}")

        # Process batch with clinical model
        results = classifier(batch, disease_labels, multi_label=False)
        batch_predictions = [res["labels"][0] for res in results]
        all_predictions.extend(batch_predictions)

    return all_predictions

# Run predictions on the small test set
small_test_df["predicted_disease"] = predict_diseases_batch(
    small_test_df["symptoms_text"].tolist(),
    top_diseases
)

# Display sample results with better formatting
print("\nSample predictions:")
results_df = small_test_df[["symptoms_text", "Disease_Name", "predicted_disease"]].head(10)
print(results_df.to_string(index=False, justify='left'))

# Calculate Accuracy
accuracy = (small_test_df["Disease_Name"] == small_test_df["predicted_disease"]).mean()
print(f"\nZero-Shot Bio_ClinicalBERT Accuracy on sample: {accuracy:.2%}")

# Enhanced partial matching (case insensitive and word boundary aware)
small_test_df["partial_match"] = small_test_df.apply(
    lambda row: (row["Disease_Name"].lower() in row["predicted_disease"].lower()) or
               (row["predicted_disease"].lower() in row["Disease_Name"].lower()) or
               any(word in row["predicted_disease"].lower().split()
                   for word in row["Disease_Name"].lower().split()),
    axis=1
)
partial_accuracy = small_test_df["partial_match"].mean()
print(f"Enhanced partial match accuracy: {partial_accuracy:.2%}")

# Generate detailed classification report
print("\nClassification Report:")
report = classification_report(
    small_test_df["Disease_Name"],
    small_test_df["predicted_disease"],
    zero_division=0,
    output_dict=False
)
print(report)

# Example prediction with confidence scores
sample_case = small_test_df.iloc[0]["symptoms_text"]
prediction = classifier(sample_case, top_diseases)
print(f"\nExample prediction for symptoms: '{sample_case}'")
print("Top 3 predicted diseases with confidence scores:")
for label, score in zip(prediction['labels'][:3], prediction['scores'][:3]):
    print(f"- {label}: {score:.2%}")

# Save results to CSV
output_cols = ["symptoms_text", "Disease_Name", "predicted_disease", "partial_match"]
small_test_df[output_cols].to_csv(
    "clinicalbert_disease_predictions.csv",
    index=False
)
print("\nResults saved to clinicalbert_disease_predictions.csv")

"""###DS4"""

import pandas as pd
import re
import json
import ast
from datasets import load_dataset
from sklearn.metrics import accuracy_score, classification_report
from sentence_transformers import SentenceTransformer, util

# Load dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")
df = pd.DataFrame(ds['train'])

# Extract disease
def extract_disease(prompt):
    match = re.search(r'Doctor-patient conversation about (.+)', prompt)
    return match.group(1).strip() if match else None

# Extract symptoms
def extract_symptoms(messages):
    try:
        if isinstance(messages, str):
            try:
                messages = json.loads(messages)
            except:
                try:
                    messages = ast.literal_eval(messages)
                except:
                    matches = re.findall(r"'role': 'patient', 'content': '(.+?)'", messages)
                    return " ".join(matches)
        symptoms = " ".join(msg.get("content", "") for msg in messages if msg.get("role") == "patient")
        return symptoms.strip()
    except:
        return ""

# Process dataset
df['disease'] = df['prompt'].apply(extract_disease)
df['symptoms'] = df['messages'].apply(extract_symptoms)
df = df.dropna(subset=['disease', 'symptoms'])
df = df[df['symptoms'] != ""]

# Sample top N diseases
top_n = 20
top_diseases = df['disease'].value_counts().head(top_n).index.tolist()
df = df[df['disease'].isin(top_diseases)]

# Sample test set
sample_size = min(50, len(df))
sample_df = df.sample(sample_size, random_state=42).reset_index(drop=True)

# Load sentence transformer model
model = SentenceTransformer("pritamdeka/Bio_ClinicalBERT_Sentence_Embedding")

# Encode disease labels
disease_embeddings = model.encode(top_diseases, convert_to_tensor=True)

# Predict using cosine similarity
predictions = []
for text in sample_df["symptoms"]:
    symptom_embedding = model.encode(text, convert_to_tensor=True)
    cosine_scores = util.pytorch_cos_sim(symptom_embedding, disease_embeddings)
    predicted_index = cosine_scores.argmax().item()
    predictions.append(top_diseases[predicted_index])

# Evaluation
sample_df["predicted_disease"] = predictions
accuracy = (sample_df["disease"] == sample_df["predicted_disease"]).mean()
print(f"\nBio_ClinicalBERT ZSL Accuracy: {accuracy:.2%}")

# Partial match
sample_df["partial_match"] = sample_df.apply(
    lambda row: row["disease"] in row["predicted_disease"] or row["predicted_disease"] in row["disease"],
    axis=1
)
partial_accuracy = sample_df["partial_match"].mean()
print(f"Partial match accuracy: {partial_accuracy:.2%}")

# Classification report
print("\nClassification Report:")
print(classification_report(sample_df["disease"], sample_df["predicted_disease"], zero_division=0))

# Save results
sample_df[["symptoms", "disease", "predicted_disease", "partial_match"]].to_csv("bio_clinicalbert_zsl_results.csv", index=False)
print("\nResults saved to bio_clinicalbert_zsl_results.csv")

"""##FSL

###DS1
"""

import pandas as pd
import numpy as np
import torch
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load dataset
ds = load_dataset("fhai50032/Symptoms_to_disease_7k")
df = pd.DataFrame(ds['train'])

# Preprocess
df["symptoms"] = df["query"].str.extract(r'Patient:I may have (.*)')
df["disease"] = df["response"].str.replace("You may have ", "", regex=False)
df = df.dropna(subset=["symptoms", "disease"])

# Few-shot setup: top 5 diseases √ó 20 examples each
top_diseases = df["disease"].value_counts().head(5).index.tolist()
fsl_df = df[df["disease"].isin(top_diseases)].groupby("disease").apply(
    lambda x: x.sample(n=min(20, len(x)), random_state=42)
).reset_index(drop=True)

# Train-test split
train_df, test_df = train_test_split(fsl_df, test_size=0.2, random_state=42)

# Hugging Face Dataset conversion
train_dataset = Dataset.from_pandas(train_df[["symptoms", "disease"]])
test_dataset = Dataset.from_pandas(test_df[["symptoms", "disease"]])

# Tokenizer and model
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(top_diseases))

# Preprocessing
label_list = train_dataset.unique("disease")
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}

def tokenize(example):
    return tokenizer(example["symptoms"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)
train_dataset = train_dataset.map(lambda x: {"label": label2id[x["disease"]]})
test_dataset = test_dataset.map(lambda x: {"label": label2id[x["disease"]]})

# Training arguments
training_args = TrainingArguments(
    output_dir="./bioclinicalbert-fsl",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    num_train_epochs=5,
    learning_rate=2e-5,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

# Metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "precision": precision_score(labels, preds, average="weighted", zero_division=0),
        "recall": recall_score(labels, preds, average="weighted", zero_division=0),
        "f1": f1_score(labels, preds, average="weighted", zero_division=0)
    }

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Evaluate
metrics = trainer.evaluate()
print("\n‚úÖ Few-Shot Learning with Bio_ClinicalBERT")
print(f"Accuracy:  {metrics['eval_accuracy']:.4f}")
print(f"Precision: {metrics['eval_precision']:.4f}")
print(f"Recall:    {metrics['eval_recall']:.4f}")
print(f"F1 Score:  {metrics['eval_f1']:.4f}")

"""###DS2"""

import pandas as pd
import numpy as np
import re
import torch
from datasets import Dataset, load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load dataset
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

# Extract symptoms and diseases
data = []
for _, row in df.iterrows():
    instruction = row['instruction']
    output = row['output']

    sym_match = re.search(r'I am having the following symptoms: (.*?)(?:\.|$)', instruction)
    dis_match = re.search(r'dealing with (.*?)(?:\.|$)', output)

    if sym_match and dis_match:
        data.append({
            "symptoms": sym_match.group(1).strip(),
            "disease": dis_match.group(1).strip()
        })

df_processed = pd.DataFrame(data)

# Few-shot: Top 5 diseases, 20 samples each
top_diseases = df_processed["disease"].value_counts().head(5).index.tolist()
fsl_df = df_processed[df_processed["disease"].isin(top_diseases)].groupby("disease").apply(
    lambda x: x.sample(n=min(20, len(x)), random_state=42)
).reset_index(drop=True)

# Train/test split
train_df, test_df = train_test_split(fsl_df, test_size=0.2, random_state=42)

# Convert to HuggingFace datasets
train_dataset = Dataset.from_pandas(train_df[["symptoms", "disease"]])
test_dataset = Dataset.from_pandas(test_df[["symptoms", "disease"]])

# Label encoding
label_list = train_dataset.unique("disease")
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}

train_dataset = train_dataset.map(lambda x: {"label": label2id[x["disease"]]})
test_dataset = test_dataset.map(lambda x: {"label": label2id[x["disease"]]})

# Tokenizer and model
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))

# Tokenize
def tokenize_fn(example):
    return tokenizer(example["symptoms"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)

# Training setup
training_args = TrainingArguments(
    output_dir="./clinicalbert-fsl",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    learning_rate=2e-5,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy"
)

# Metric computation
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "precision": precision_score(labels, preds, average="weighted", zero_division=0),
        "recall": recall_score(labels, preds, average="weighted", zero_division=0),
        "f1": f1_score(labels, preds, average="weighted", zero_division=0)
    }

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Evaluate
metrics = trainer.evaluate()
print("\n‚úÖ Few-Shot Learning Evaluation (Bio_ClinicalBERT):")
print(f"Accuracy : {metrics['eval_accuracy']:.4f}")
print(f"Precision: {metrics['eval_precision']:.4f}")
print(f"Recall   : {metrics['eval_recall']:.4f}")
print(f"F1 Score : {metrics['eval_f1']:.4f}")

"""###DS3"""

import pandas as pd
import ast
import numpy as np
import torch
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

# Load data
df = pd.read_parquet("hf://datasets/Mohamed-Ahmed161/Disease-Symptoms/data/train-00000-of-00001.parquet")

# Extract symptoms
def extract_symptoms(symptom_json_str):
    try:
        d = ast.literal_eval(symptom_json_str)
        return ", ".join(d.get("symptoms", []))
    except:
        return ""

df["symptoms_text"] = df["Symptom_json"].apply(extract_symptoms)
df = df[df["symptoms_text"] != ""]

# Select top 5 diseases for few-shot setting
top_diseases = df["Disease_Name"].value_counts().head(5).index.tolist()
few_shot_df = df[df["Disease_Name"].isin(top_diseases)]

# Sample 20 examples per class
fsl_df = few_shot_df.groupby("Disease_Name").apply(lambda x: x.sample(n=min(20, len(x)), random_state=42)).reset_index(drop=True)

# Train-test split
train_df, test_df = train_test_split(fsl_df, test_size=0.2, stratify=fsl_df["Disease_Name"], random_state=42)

# Convert to HuggingFace dataset
train_dataset = Dataset.from_pandas(train_df[["symptoms_text", "Disease_Name"]])
test_dataset = Dataset.from_pandas(test_df[["symptoms_text", "Disease_Name"]])

# Label encoding
label_list = sorted(train_dataset.unique("Disease_Name"))
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}

train_dataset = train_dataset.map(lambda x: {"label": label2id[x["Disease_Name"]]})
test_dataset = test_dataset.map(lambda x: {"label": label2id[x["Disease_Name"]]})

# Tokenization
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))

def tokenize_fn(example):
    return tokenizer(example["symptoms_text"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./fsl-clinicalbert",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=5,
    evaluation_strategy="epoch",
    save_strategy="no",
    logging_strategy="epoch",
    learning_rate=2e-5,
    seed=42
)

# Metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "precision": precision_score(labels, preds, average="weighted", zero_division=0),
        "recall": recall_score(labels, preds, average="weighted", zero_division=0),
        "f1": f1_score(labels, preds, average="weighted", zero_division=0)
    }

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Evaluate
metrics = trainer.evaluate()
print("\n‚úÖ Few-Shot Learning Evaluation Results:")
print(f"Accuracy : {metrics['eval_accuracy']:.4f}")
print(f"Precision: {metrics['eval_precision']:.4f}")
print(f"Recall   : {metrics['eval_recall']:.4f}")
print(f"F1 Score : {metrics['eval_f1']:.4f}")

"""###DS4"""

import pandas as pd
import re
import json
import ast
import numpy as np
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load dataset
from datasets import load_dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")
df = pd.DataFrame(ds['train'])

# Extract disease
def extract_disease(prompt):
    match = re.search(r'Doctor-patient conversation about (.+)', prompt)
    return match.group(1).strip() if match else None

# Extract symptoms
def extract_symptoms(messages):
    try:
        if isinstance(messages, str):
            try:
                messages = json.loads(messages)
            except:
                try:
                    messages = ast.literal_eval(messages)
                except:
                    matches = re.findall(r"'role': 'patient', 'content': '(.+?)'", messages)
                    return " ".join(matches)
        symptoms = " ".join(msg.get("content", "") for msg in messages if msg.get("role") == "patient")
        return symptoms.strip()
    except:
        return ""

# Process data
df['disease'] = df['prompt'].apply(extract_disease)
df['symptoms'] = df['messages'].apply(extract_symptoms)
df = df.dropna(subset=['disease', 'symptoms'])
df = df[df['symptoms'] != ""]

# Select top N diseases for FSL
top_n = 5
top_diseases = df['disease'].value_counts().head(top_n).index.tolist()
df = df[df['disease'].isin(top_diseases)]

# Sample few-shot examples per class
fsl_df = df.groupby("disease").apply(lambda x: x.sample(n=min(20, len(x)), random_state=42)).reset_index(drop=True)

# Train-test split
train_df, test_df = train_test_split(fsl_df, test_size=0.2, stratify=fsl_df['disease'], random_state=42)

# Encode labels
label_list = sorted(train_df["disease"].unique())
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}

# Convert to HF dataset
train_dataset = Dataset.from_pandas(train_df[["symptoms", "disease"]])
test_dataset = Dataset.from_pandas(test_df[["symptoms", "disease"]])

train_dataset = train_dataset.map(lambda x: {"label": label2id[x["disease"]]})
test_dataset = test_dataset.map(lambda x: {"label": label2id[x["disease"]]})

# Load model & tokenizer
model_name = "emilyalsentzer/Bio_ClinicalBERT"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id))

# Tokenize data
def tokenize_fn(example):
    return tokenizer(example["symptoms"], padding="max_length", truncation=True, max_length=128)

train_dataset = train_dataset.map(tokenize_fn, batched=True)
test_dataset = test_dataset.map(tokenize_fn, batched=True)

# Training configuration
training_args = TrainingArguments(
    output_dir="./fsl-clinicalbert-chat",
    num_train_epochs=5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy="epoch",
    save_strategy="no",
    logging_strategy="epoch",
    learning_rate=2e-5,
    seed=42
)

# Evaluation metrics
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "precision": precision_score(labels, preds, average="weighted", zero_division=0),
        "recall": recall_score(labels, preds, average="weighted", zero_division=0),
        "f1": f1_score(labels, preds, average="weighted", zero_division=0)
    }

# Initialize trainer
trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Evaluate
metrics = trainer.evaluate()
print("\n‚úÖ Few-Shot Learning Evaluation:")
print(f"Accuracy : {metrics['eval_accuracy']:.4f}")
print(f"Precision: {metrics['eval_precision']:.4f}")
print(f"Recall   : {metrics['eval_recall']:.4f}")
print(f"F1 Score : {metrics['eval_f1']:.4f}")

"""#FLAN-TF

##ZSL

###DS1
"""

!pip install transformers datasets sacremoses # Install dependencies

import pandas as pd
import numpy as np
from datasets import load_dataset
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import torch

# Load dataset from Hugging Face
ds = load_dataset("fhai50032/Symptoms_to_disease_7k")
df = pd.DataFrame(ds['train'])

# Extract symptoms and diseases
df["symptoms"] = df["query"].str.extract(r'Patient:I may have (.*)')
df["disease"] = df["response"].str.replace("You may have ", "", regex=False)

# Drop missing values
df = df.dropna(subset=["symptoms", "disease"])

# Split Dataset (80% Train, 20% Test for evaluation)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

print(f"Training set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")

# Load FLAN-TF model and tokenizer
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Function to create zero-shot prompts for FLAN-TF
def create_zero_shot_prompt(symptoms, disease_labels):
    return f"""Given the following symptoms: {symptoms}
Which disease from the options below is most likely?

Options: {', '.join(disease_labels)}
Answer with the disease name only."""

# Function for batch inference using FLAN-TF
def predict_diseases_zero_shot(symptom_list, disease_labels, batch_size=8):
    all_predictions = []

    for i in range(0, len(symptom_list), batch_size):
        batch = symptom_list[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(symptom_list) + batch_size - 1)//batch_size}")

        # Create prompts for the batch
        prompts = [create_zero_shot_prompt(symptoms, disease_labels) for symptoms in batch]

        # Tokenize and generate predictions
        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=20,
                temperature=0.1,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )

        # Decode and clean predictions
        batch_predictions = []
        for j in range(len(outputs)):
            text = tokenizer.decode(outputs[j], skip_special_tokens=True)
            # Extract just the answer part
            answer = text.replace(prompts[j], "").strip()
            # Clean and remove extra characters
            answer = answer.split('\n')[0].strip().rstrip('.').strip()
            batch_predictions.append(answer)

        all_predictions.extend(batch_predictions)

    return all_predictions

# Create a smaller test set for demonstration (50 samples)
small_test_df = test_df.sample(50, random_state=42)

# Apply zero-shot inference to the small test set
small_test_df["predicted_disease"] = predict_diseases_zero_shot(
    small_test_df["symptoms"].tolist(),
    top_diseases
)

# Display sample results
print("\nSample predictions:")
results_df = small_test_df[["symptoms", "disease", "predicted_disease"]].head(10)
print(results_df)

# Calculate Accuracy
accuracy = (small_test_df["disease"] == small_test_df["predicted_disease"]).mean()
print(f"\nFLAN-TF Accuracy on sample: {accuracy:.2%}")

# Check for partial matches (disease names containing parts of predicted disease names)
small_test_df["partial_match"] = small_test_df.apply(
    lambda row: row["disease"].lower() in row["predicted_disease"].lower() or
               row["predicted_disease"].lower() in row["disease"].lower(),
    axis=1
)
partial_accuracy = small_test_df["partial_match"].mean()
print(f"Partial match accuracy: {partial_accuracy:.2%}")

# Generate a classification report for the diseases in our sample
print("\nClassification Report:")
# Filter to only diseases that appear in our small test set for the report
present_diseases = small_test_df["disease"].unique()
relevant_mask = small_test_df["disease"].isin(present_diseases)
report = classification_report(
    small_test_df.loc[relevant_mask, "disease"],
    small_test_df.loc[relevant_mask, "predicted_disease"],
    zero_division=0
)
print(report)

"""###DS2"""

import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import re
import torch

# Load the dataset
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

# Process only "symptoms ‚Üí disease" format
processed_data = []
for _, row in df.iterrows():
    instruction, output = row['instruction'], row['output']
    if "I am having the following symptoms:" in instruction and "What could be the disease" in instruction:
        symptoms_match = re.search(r'I am having the following symptoms: (.*?)(?:\.|$)', instruction)
        disease_match = re.search(r'dealing with (.*?)(?:\.|$)', output)
        if symptoms_match and disease_match:
            processed_data.append({
                "symptoms": symptoms_match.group(1).strip(),
                "disease": disease_match.group(1).strip()
            })

df_proc = pd.DataFrame(processed_data)
print(f"Processed dataset size: {len(df_proc)}")

# Sample 50 examples for quick evaluation
test_df = df_proc.sample(min(50, len(df_proc)), random_state=42).reset_index(drop=True)

# Load FLAN-T5
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Move to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# ZSL prediction function
def predict_disease(symptoms):
    prompt = f"A patient has the following symptoms: {symptoms}. What could be the disease?"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True).to(device)
    outputs = model.generate(**inputs, max_new_tokens=20)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

# Run predictions
print("Generating predictions using FLAN-T5...")
test_df["predicted_disease"] = [predict_disease(s) for s in tqdm(test_df["symptoms"])]

# Display results
print("\nSample predictions:")
print(test_df[["symptoms", "disease", "predicted_disease"]].head(10))

# Accuracy (Exact match)
exact_match_acc = (test_df["disease"] == test_df["predicted_disease"]).mean()
print(f"\nFLAN-T5 Zero-Shot Accuracy (Exact Match): {exact_match_acc:.2%}")

# Partial match
test_df["partial_match"] = test_df.apply(
    lambda r: r["disease"].lower() in r["predicted_disease"].lower()
              or r["predicted_disease"].lower() in r["disease"].lower(),
    axis=1
)
partial_match_acc = test_df["partial_match"].mean()
print(f"FLAN-T5 Zero-Shot Partial Match Accuracy: {partial_match_acc:.2%}")

"""###DS3"""

import pandas as pd
import ast
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load data
df = pd.read_parquet("hf://datasets/Mohamed-Ahmed161/Disease-Symptoms/data/train-00000-of-00001.parquet")

# Extract symptoms
def extract_symptoms(symptom_json_str):
    try:
        d = ast.literal_eval(symptom_json_str)
        return ", ".join(d.get("symptoms", []))
    except:
        return ""

df["symptoms_text"] = df["Symptom_json"].apply(extract_symptoms)
df = df[df["symptoms_text"] != ""]

# Select top 5 diseases for candidate label space
top_diseases = df["Disease_Name"].value_counts().head(5).index.tolist()
df = df[df["Disease_Name"].isin(top_diseases)]

# Sample test set
sample_df = df.groupby("Disease_Name").apply(lambda x: x.sample(n=min(10, len(x)), random_state=42)).reset_index(drop=True)

# Load FLAN-T5
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# Format prompts
def format_prompt(symptoms, candidates):
    return f"Given the following symptoms: {symptoms}\nWhich of the following diseases is most likely?\nOptions: {', '.join(candidates)}\nAnswer:"

# Generate predictions
predictions = []
for _, row in sample_df.iterrows():
    prompt = format_prompt(row["symptoms_text"], top_diseases)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    output = model.generate(**inputs, max_new_tokens=10)
    pred = tokenizer.decode(output[0], skip_special_tokens=True).strip()
    predictions.append(pred)

# Evaluation
sample_df["predicted_disease"] = predictions

# Clean predictions (handle partial match)
def clean_match(pred, candidates):
    pred_lower = pred.lower()
    for c in candidates:
        if c.lower() in pred_lower or pred_lower in c.lower():
            return c
    return pred

sample_df["predicted_disease"] = sample_df["predicted_disease"].apply(lambda x: clean_match(x, top_diseases))

# Metrics
y_true = sample_df["Disease_Name"]
y_pred = sample_df["predicted_disease"]

print("\n‚úÖ Zero-Shot Learning Evaluation with FLAN-T5:")
print(f"Accuracy : {accuracy_score(y_true, y_pred):.4f}")
print(f"Precision: {precision_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")
print(f"Recall   : {recall_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")
print(f"F1 Score : {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")

"""###DS4"""

import pandas as pd
import re
import json
import ast
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load dataset
from datasets import load_dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")
df = pd.DataFrame(ds['train'])

# Extract disease and symptoms
def extract_disease(prompt):
    match = re.search(r'Doctor-patient conversation about (.+)', prompt)
    return match.group(1).strip() if match else None

def extract_symptoms(messages):
    try:
        if isinstance(messages, str):
            try:
                messages = json.loads(messages)
            except:
                try:
                    messages = ast.literal_eval(messages)
                except:
                    matches = re.findall(r"'role': 'patient', 'content': '(.+?)'", messages)
                    return " ".join(matches)
        symptoms = " ".join(msg.get("content", "") for msg in messages if msg.get("role") == "patient")
        return symptoms.strip()
    except:
        return ""

df["disease"] = df["prompt"].apply(extract_disease)
df["symptoms"] = df["messages"].apply(extract_symptoms)
df = df.dropna(subset=["disease", "symptoms"])
df = df[df["symptoms"] != ""]

# Use top-N diseases as candidate labels
top_n = 5
top_diseases = df["disease"].value_counts().head(top_n).index.tolist()
df = df[df["disease"].isin(top_diseases)]

# Create a few test samples
test_df = df.groupby("disease").head(3).reset_index(drop=True)

# Load FLAN-T5
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# Construct ZSL prompt and run inference
def build_zsl_prompt(symptoms, label_options):
    options_str = ", ".join(label_options)
    prompt = f"The patient has the following symptoms: {symptoms}\nWhich disease best matches these symptoms? Options: {options_str}\nAnswer:"
    return prompt

predictions = []
for _, row in test_df.iterrows():
    prompt = build_zsl_prompt(row["symptoms"], top_diseases)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
    outputs = model.generate(**inputs, max_new_tokens=10)
    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    predictions.append(pred)

# Match predicted text to closest valid label
def clean_pred(pred, labels):
    pred = pred.lower()
    for label in labels:
        if label.lower() in pred or pred in label.lower():
            return label
    return pred

test_df["predicted"] = [clean_pred(p, top_diseases) for p in predictions]

# Evaluation
y_true = test_df["disease"]
y_pred = test_df["predicted"]

print("\n‚úÖ FLAN-T5 Zero-Shot Learning Evaluation:")
print(f"Accuracy : {accuracy_score(y_true, y_pred):.4f}")
print(f"Precision: {precision_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")
print(f"Recall   : {recall_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")
print(f"F1 Score : {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")

"""##FSL

###DS1
"""

!pip install transformers datasets sacremoses  # Install dependencies

import pandas as pd
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import torch

# Load dataset from Hugging Face
ds = load_dataset("fhai50032/Symptoms_to_disease_7k")
df = pd.DataFrame(ds['train'])

# Extract symptoms and diseases
df["symptoms"] = df["query"].str.extract(r'Patient:I may have (.*)')
df["disease"] = df["response"].str.replace("You may have ", "", regex=False)

# Drop missing values
df = df.dropna(subset=["symptoms", "disease"])

# Split Dataset (80% Train, 20% Test for evaluation)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

print(f"Training set size: {len(train_df)}")
print(f"Test set size: {len(test_df)}")

# Load FLAN-TF model and tokenizer
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Sample a few examples from each disease for Few-Shot Learning (FSL)
few_shot_examples = {}

# For each unique disease, select 1 or a few examples
for disease in df['disease'].unique():
    sample = df[df['disease'] == disease].sample(1)  # Choose 1 example per disease for FSL
    few_shot_examples[disease] = sample["symptoms"].values[0]

# Generate FSL prompts for each disease using few examples
def create_few_shot_prompt(symptoms, disease_examples, disease_labels):
    few_shot_prompt = "\n".join([f"Symptom: {example}\nDisease: {disease}" for disease, example in disease_examples.items()])
    prompt = f"""Given the following few-shot examples:
{few_shot_prompt}

Now, based on these symptoms: {symptoms}
Which disease from the options below is most likely?

Options: {', '.join(disease_labels)}
Answer with the disease name only."""
    return prompt

# Function for batch inference using FLAN-TF with Few-Shot Learning
def predict_diseases_fsl(symptom_list, disease_labels, disease_examples, batch_size=8):
    all_predictions = []

    for i in range(0, len(symptom_list), batch_size):
        batch = symptom_list[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}/{(len(symptom_list) + batch_size - 1)//batch_size}")

        # Create prompts for the batch
        prompts = [create_few_shot_prompt(symptoms, disease_examples, disease_labels) for symptoms in batch]

        # Tokenize and generate predictions
        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=512).to(device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=20,
                temperature=0.1,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )

        # Decode and clean predictions
        batch_predictions = []
        for j in range(len(outputs)):
            text = tokenizer.decode(outputs[j], skip_special_tokens=True)
            # Extract just the answer part
            answer = text.replace(prompts[j], "").strip()
            # Clean and remove extra characters
            answer = answer.split('\n')[0].strip().rstrip('.').strip()
            batch_predictions.append(answer)

        all_predictions.extend(batch_predictions)

    return all_predictions

# Create a smaller test set for demonstration (50 samples)
small_test_df = test_df.sample(50, random_state=42)

# Apply Few-Shot Learning inference to the small test set
small_test_df["predicted_disease"] = predict_diseases_fsl(
    small_test_df["symptoms"].tolist(),
    top_diseases,
    few_shot_examples
)

# Display sample results
print("\nSample predictions:")
results_df = small_test_df[["symptoms", "disease", "predicted_disease"]].head(10)
print(results_df)

# Calculate Accuracy
accuracy = (small_test_df["disease"] == small_test_df["predicted_disease"]).mean()
print(f"\nFLAN-TF FSL Accuracy on sample: {accuracy:.2%}")

# Check for partial matches (disease names containing parts of predicted disease names)
small_test_df["partial_match"] = small_test_df.apply(
    lambda row: row["disease"].lower() in row["predicted_disease"].lower() or
               row["predicted_disease"].lower() in row["disease"].lower(),
    axis=1
)
partial_accuracy = small_test_df["partial_match"].mean()
print(f"Partial match accuracy: {partial_accuracy:.2%}")

# Generate a classification report for the diseases in our sample
print("\nClassification Report:")
# Filter to only diseases that appear in our small test set for the report
present_diseases = small_test_df["disease"].unique()
relevant_mask = small_test_df["disease"].isin(present_diseases)
report = classification_report(
    small_test_df.loc[relevant_mask, "disease"],
    small_test_df.loc[relevant_mask, "predicted_disease"],
    zero_division=0
)
print(report)

"""###DS2"""

import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import re
import torch

# Load and preprocess the dataset
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

processed_data = []
for _, row in df.iterrows():
    instr, outp = row['instruction'], row['output']
    if "I am having the following symptoms:" in instr and "What could be the disease" in instr:
        s_match = re.search(r'I am having the following symptoms: (.*?)(?:\.|$)', instr)
        d_match = re.search(r'dealing with (.*?)(?:\.|$)', outp)
        if s_match and d_match:
            processed_data.append({
                "symptoms": s_match.group(1).strip(),
                "disease": d_match.group(1).strip()
            })

df_proc = pd.DataFrame(processed_data)
print(f"Processed dataset size: {len(df_proc)}")

# Few-shot: pick N examples
N = 5
few_shot_examples = df_proc.sample(N, random_state=42)

# Create test set (excluding few-shot examples)
remaining_df = df_proc.drop(few_shot_examples.index)
test_df = remaining_df.sample(min(50, len(remaining_df)), random_state=42).reset_index(drop=True)

# Build few-shot prompt template
few_shot_prompt = ""
for _, row in few_shot_examples.iterrows():
    few_shot_prompt += f"Q: A patient has the following symptoms: {row['symptoms']}. What could be the disease?\n"
    few_shot_prompt += f"A: {row['disease']}\n\n"

# Load FLAN-T5
model_name = "google/flan-t5-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Inference function
def predict_fsl(symptoms):
    prompt = few_shot_prompt + f"Q: A patient has the following symptoms: {symptoms}. What could be the disease?\nA:"
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, padding=True).to(device)
    outputs = model.generate(**inputs, max_new_tokens=20)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

# Generate predictions
print("Generating predictions with FLAN-T5 FSL...")
test_df["predicted_disease"] = [predict_fsl(s) for s in tqdm(test_df["symptoms"])]

# Show sample predictions
print("\nSample Predictions:")
print(test_df[["symptoms", "disease", "predicted_disease"]].head(10))

# Exact match accuracy
exact_match_acc = (test_df["disease"] == test_df["predicted_disease"]).mean()
print(f"\nFLAN-T5 Few-Shot Accuracy (Exact Match): {exact_match_acc:.2%}")

# Partial match
test_df["partial_match"] = test_df.apply(
    lambda r: r["disease"].lower() in r["predicted_disease"].lower()
              or r["predicted_disease"].lower() in r["disease"].lower(),
    axis=1
)
partial_acc = test_df["partial_match"].mean()
print(f"FLAN-T5 Few-Shot Accuracy (Partial Match): {partial_acc:.2%}")

"""###DS3"""

import pandas as pd
import ast
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load and process dataset
df = pd.read_parquet("hf://datasets/Mohamed-Ahmed161/Disease-Symptoms/data/train-00000-of-00001.parquet")

def extract_symptoms(s):
    try:
        d = ast.literal_eval(s)
        return ", ".join(d.get("symptoms", []))
    except:
        return ""

df["symptoms_text"] = df["Symptom_json"].apply(extract_symptoms)
df = df[df["symptoms_text"] != ""]

# Select top 5 diseases
top_diseases = df["Disease_Name"].value_counts().head(5).index.tolist()
df = df[df["Disease_Name"].isin(top_diseases)]

# Sample few-shot examples: 4 per class for prompt, 1 per class for test
few_shot_df = df.groupby("Disease_Name").apply(lambda x: x.sample(n=5, random_state=42)).reset_index(drop=True)

shots_df = few_shot_df.groupby("Disease_Name").head(4)
test_df = few_shot_df.groupby("Disease_Name").tail(1).reset_index(drop=True)

# Build in-context prompt
def build_prompt(symptoms, shots):
    prompt = ""
    for _, row in shots.iterrows():
        prompt += f"Symptoms: {row['symptoms_text']}\nDiagnosis: {row['Disease_Name']}\n\n"
    prompt += f"Symptoms: {symptoms}\nDiagnosis:"
    return prompt

# Load model
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# Predict
predictions = []
for _, row in test_df.iterrows():
    prompt = build_prompt(row["symptoms_text"], shots_df)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
    outputs = model.generate(**inputs, max_new_tokens=10)
    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    predictions.append(pred)

# Post-process predictions: match to closest label
def clean_pred(pred, labels):
    pred_lower = pred.lower()
    for l in labels:
        if l.lower() in pred_lower or pred_lower in l.lower():
            return l
    return pred

test_df["predicted_disease"] = [clean_pred(p, top_diseases) for p in predictions]

# Metrics
y_true = test_df["Disease_Name"]
y_pred = test_df["predicted_disease"]

print("\n‚úÖ FLAN-T5 Few-Shot Learning Evaluation:")
print(f"Accuracy : {accuracy_score(y_true, y_pred):.4f}")
print(f"Precision: {precision_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")
print(f"Recall   : {recall_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")
print(f"F1 Score : {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")

"""###DS4"""

import pandas as pd
import re
import json
import ast
import numpy as np
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Load dataset
from datasets import load_dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")
df = pd.DataFrame(ds['train'])

# Extract disease
def extract_disease(prompt):
    match = re.search(r'Doctor-patient conversation about (.+)', prompt)
    return match.group(1).strip() if match else None

# Extract symptoms
def extract_symptoms(messages):
    try:
        if isinstance(messages, str):
            try:
                messages = json.loads(messages)
            except:
                try:
                    messages = ast.literal_eval(messages)
                except:
                    matches = re.findall(r"'role': 'patient', 'content': '(.+?)'", messages)
                    return " ".join(matches)
        symptoms = " ".join(msg.get("content", "") for msg in messages if msg.get("role") == "patient")
        return symptoms.strip()
    except:
        return ""

df['disease'] = df['prompt'].apply(extract_disease)
df['symptoms'] = df['messages'].apply(extract_symptoms)
df = df.dropna(subset=['disease', 'symptoms'])
df = df[df['symptoms'] != ""]

# Select top N diseases for FSL
top_n = 5
top_diseases = df['disease'].value_counts().head(top_n).index.tolist()
df = df[df['disease'].isin(top_diseases)]

# Sample few-shot examples per class
fsl_df = df.groupby("disease").apply(lambda x: x.sample(n=min(20, len(x)), random_state=42)).reset_index(drop=True)

# Train-test split
train_df, test_df = train_test_split(fsl_df, test_size=0.2, stratify=fsl_df['disease'], random_state=42)

# Encode labels
label_list = sorted(train_df["disease"].unique())
label2id = {label: i for i, label in enumerate(label_list)}
id2label = {i: label for label, i in label2id.items()}

# Convert to HF dataset
train_dataset = Dataset.from_pandas(train_df[["symptoms", "disease"]])
test_dataset = Dataset.from_pandas(test_df[["symptoms", "disease"]])

train_dataset = train_dataset.map(lambda x: {"label": label2id[x["disease"]]}).remove_columns(['disease'])
test_dataset = test_dataset.map(lambda x: {"label": label2id[x["disease"]]}).remove_columns(['disease'])

# Load FLAN-T5 model and tokenizer
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# Function to create few-shot prompt for FLAN-T5
def create_few_shot_prompt(symptoms, disease_labels, n_shots=5):
    # Create few-shot examples
    few_shot_examples = []
    for disease in disease_labels[:n_shots]:
        example = f"Symptom: {symptoms}\nDisease: {disease}"
        few_shot_examples.append(example)

    # Combine few-shot examples and final prompt
    few_shot_prompt = "\n".join(few_shot_examples) + f"\nSymptom: {symptoms}\nDisease:"
    return few_shot_prompt

# Generate predictions for the test set
predictions = []
for _, row in test_df.iterrows():
    prompt = create_few_shot_prompt(row["symptoms"], top_diseases, n_shots=5)
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
    outputs = model.generate(**inputs, max_new_tokens=10)
    pred = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
    predictions.append(pred)

# Match predicted text to closest valid label
def clean_pred(pred, labels):
    pred = pred.lower()
    for label in labels:
        if label.lower() in pred or pred in label.lower():
            return label
    return pred

test_df["predicted"] = [clean_pred(p, top_diseases) for p in predictions]

# Evaluation
y_true = test_df["disease"]
y_pred = test_df["predicted"]

print("\n‚úÖ FLAN-T5 Few-Shot Learning Evaluation:")
print(f"Accuracy : {accuracy_score(y_true, y_pred):.4f}")
print(f"Precision: {precision_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")
print(f"Recall   : {recall_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")
print(f"F1 Score : {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}")

"""#CNN

##ZSL

###DS1
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report

# Prepare dataset for CNN
class DiseaseDataset(Dataset):
    def __init__(self, symptoms, labels, tokenizer, max_len=512):
        self.symptoms = symptoms
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.symptoms)

    def __getitem__(self, idx):
        symptom = self.symptoms[idx]
        label = self.labels[idx]
        # Tokenize symptoms using the BioGPT tokenizer
        encoding = self.tokenizer(
            symptom,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()
        return input_ids, attention_mask, label


# CNN Model for Disease Prediction (ZSL Approach)
class CNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes, kernel_sizes=[3, 4, 5], num_filters=128):
        super(CNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # Convolutional layers with different kernel sizes
        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filters, (size, embedding_dim)) for size in kernel_sizes
        ])

        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)

    def forward(self, x):
        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)
        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len, embedding_dim)

        # Apply convolutions for each kernel size
        conv_results = [conv(x) for conv in self.convs]
        conv_results = [torch.relu(res.squeeze(3)) for res in conv_results]  # Remove the last dim (embedding_dim)

        # Pooling (max pool over the sequence length)
        pooled_results = [torch.max(res, dim=2)[0] for res in conv_results]

        # Concatenate pooled results
        x = torch.cat(pooled_results, dim=1)
        x = self.fc(x)  # Final classification layer

        return x


# Preprocess the dataset
tokenizer = AutoTokenizer.from_pretrained("microsoft/biogpt")
df["symptoms"] = df["query"].str.extract(r'Patient:I may have (.*)')
df["disease"] = df["response"].str.replace("You may have ", "", regex=False)

# Drop missing values
df = df.dropna(subset=["symptoms", "disease"])

# Encoding diseases as labels
label_encoder = LabelEncoder()
df["disease_encoded"] = label_encoder.fit_transform(df["disease"])

# Split dataset
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Create datasets and dataloaders
train_dataset = DiseaseDataset(train_df["symptoms"].tolist(), train_df["disease_encoded"].tolist(), tokenizer)
test_dataset = DiseaseDataset(test_df["symptoms"].tolist(), test_df["disease_encoded"].tolist(), tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define the model, loss function, and optimizer
vocab_size = len(tokenizer)
embedding_dim = 768  # Use the output size of BioGPT's token embeddings
num_classes = len(label_encoder.classes_)
model = CNNModel(vocab_size, embedding_dim, num_classes)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Loss and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training loop
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_preds = 0
    for inputs, masks, labels in train_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        correct_preds += (predicted == labels).sum().item()
        total_preds += labels.size(0)

    train_accuracy = correct_preds / total_preds * 100
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%")

# Evaluate on test set
model.eval()
y_true = []
y_pred = []
with torch.no_grad():
    for inputs, masks, labels in test_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Calculate accuracy, precision, recall, F1-score
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
print(f"Test Accuracy: {accuracy:.2f}%")
print(f"Test Precision: {precision:.2f}")
print(f"Test Recall: {recall:.2f}")
print(f"Test F1 Score: {f1:.2f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

"""###DS2"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from datasets import load_dataset
from sklearn.model_selection import train_test_split
import re
from collections import Counter
from tqdm import tqdm
import random

# --------------------------
# Load & preprocess dataset
# --------------------------
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

processed = []
for _, row in df.iterrows():
    i, o = row['instruction'], row['output']
    s_match = re.search(r'I am having the following symptoms: (.*?)(?:\.|$)', i)
    d_match = re.search(r'dealing with (.*?)(?:\.|$)', o)
    if s_match and d_match:
        processed.append({"symptoms": s_match.group(1).strip().lower(), "disease": d_match.group(1).strip().lower()})

df = pd.DataFrame(processed)
label_encoder = LabelEncoder()
df["label"] = label_encoder.fit_transform(df["disease"])
all_diseases = df["disease"].unique()
random.shuffle(all_diseases)

# Split seen/unseen diseases
split_point = int(0.7 * len(all_diseases))
seen_diseases = all_diseases[:split_point]
unseen_diseases = all_diseases[split_point:]

train_df = df[df["disease"].isin(seen_diseases)].reset_index(drop=True)
test_df = df[df["disease"].isin(unseen_diseases)].reset_index(drop=True)

# --------------------------
# Tokenization and encoding
# --------------------------
def tokenize(text):
    return text.lower().split()

# Build vocab from training data only
token_counts = Counter()
for text in train_df["symptoms"]:
    token_counts.update(tokenize(text))

vocab = {word: i+2 for i, (word, _) in enumerate(token_counts.most_common())}
vocab["<PAD>"] = 0
vocab["<UNK>"] = 1

def encode(text, max_len=30):
    tokens = tokenize(text)
    ids = [vocab.get(t, vocab["<UNK>"]) for t in tokens]
    return ids[:max_len] + [0] * (max_len - len(ids[:max_len]))

# --------------------------
# Dataset and CNN Model
# --------------------------
class SymptomDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = [encode(t) for t in texts]
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])

class CNNEncoder(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.conv = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveMaxPool1d(1)

    def forward(self, x):
        x = self.embedding(x).transpose(1, 2)  # B x E x T
        x = F.relu(self.conv(x))
        x = self.pool(x).squeeze(-1)  # B x 128
        return x

# --------------------------
# Train on seen diseases
# --------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
embed_dim = 100

encoder = CNNEncoder(len(vocab), embed_dim).to(device)

# Get train data
train_labels = train_df["disease"].values
train_label_ids = LabelEncoder().fit_transform(train_labels)
train_ds = SymptomDataset(train_df["symptoms"].tolist(), train_label_ids)
train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)

clf = nn.Linear(128, len(set(train_label_ids))).to(device)
opt = torch.optim.Adam(list(encoder.parameters()) + list(clf.parameters()), lr=1e-3)
loss_fn = nn.CrossEntropyLoss()

for epoch in range(5):
    encoder.train()
    clf.train()
    total_loss = 0
    for x, y in train_loader:
        x, y = x.to(device), y.to(device)
        opt.zero_grad()
        emb = encoder(x)
        logits = clf(emb)
        loss = loss_fn(logits, y)
        loss.backward()
        opt.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1} - Loss: {total_loss:.4f}")

# --------------------------
# Zero-Shot Evaluation
# --------------------------
encoder.eval()

# Create embeddings for disease names (unseen) via CNN encoder
disease_texts = [f"symptoms of {d}" for d in unseen_diseases]
disease_ids = [encode(t) for t in disease_texts]
disease_tensor = torch.tensor(disease_ids).to(device)
with torch.no_grad():
    disease_embs = encoder(disease_tensor)  # (num_unseen, 128)

# Classify test symptoms by comparing cosine similarity to disease embeddings
correct = 0
total = 0

for _, row in tqdm(test_df.iterrows(), total=len(test_df)):
    symptom_text = row["symptoms"]
    true_disease = row["disease"]

    x = torch.tensor([encode(symptom_text)]).to(device)
    with torch.no_grad():
        symptom_emb = encoder(x)  # (1, 128)

    # Cosine similarity with unseen disease embeddings
    sim = F.cosine_similarity(symptom_emb, disease_embs)  # (num_unseen,)
    pred_idx = torch.argmax(sim).item()
    pred_disease = unseen_diseases[pred_idx]

    if pred_disease == true_disease:
        correct += 1
    total += 1

zsl_acc = correct / total
print(f"\nZSL CNN Accuracy on unseen diseases: {zsl_acc:.2%}")

"""###DS3"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from transformers import AutoTokenizer
import ast

# Prepare dataset for CNN (Zero-Shot Setup)
class DiseaseSymptomDataset(Dataset):
    def __init__(self, symptoms, labels, tokenizer, max_len=512):
        self.symptoms = symptoms
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.symptoms)

    def __getitem__(self, idx):
        symptom = self.symptoms[idx]
        label = self.labels[idx]
        # Tokenize symptoms using the tokenizer
        encoding = self.tokenizer(
            symptom,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()
        return input_ids, attention_mask, label


# CNN Model for Disease Prediction
class CNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes, kernel_sizes=[3, 4, 5], num_filters=128):
        super(CNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # Convolutional layers with different kernel sizes
        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filters, (size, embedding_dim)) for size in kernel_sizes
        ])

        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)

    def forward(self, x):
        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)
        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len, embedding_dim)

        # Apply convolutions for each kernel size
        conv_results = [conv(x) for conv in self.convs]
        conv_results = [torch.relu(res.squeeze(3)) for res in conv_results]  # Remove the last dim (embedding_dim)

        # Pooling (max pool over the sequence length)
        pooled_results = [torch.max(res, dim=2)[0] for res in conv_results]

        # Concatenate pooled results
        x = torch.cat(pooled_results, dim=1)
        x = self.fc(x)  # Final classification layer

        return x


# Data Preprocessing (handling symptom json column)
def extract_symptoms(symptom_json_str):
    try:
        # Parse the JSON string
        symptom_dict = ast.literal_eval(symptom_json_str)
        symptoms = symptom_dict.get('symptoms', [])
        return " | ".join(symptoms) if symptoms else ""
    except:
        return ""


# Load dataset
ds = load_dataset("Mohamed-Ahmed161/Disease-Symptoms")
df = pd.DataFrame(ds['train'])

# Extract symptoms text
df['symptoms_text'] = df['Symptom_json'].apply(extract_symptoms)
df = df[df['symptoms_text'] != ""]

# Encode disease labels
label_encoder = LabelEncoder()
df["Disease_Name_encoded"] = label_encoder.fit_transform(df["Disease_Name"])

# Split dataset
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Initialize tokenizer and prepare datasets
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
train_dataset = DiseaseSymptomDataset(train_df["symptoms_text"].tolist(), train_df["Disease_Name_encoded"].tolist(), tokenizer)
test_dataset = DiseaseSymptomDataset(test_df["symptoms_text"].tolist(), test_df["Disease_Name_encoded"].tolist(), tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define the model
vocab_size = len(tokenizer)
embedding_dim = 768  # Use BERT-like embedding dimensions
num_classes = len(label_encoder.classes_)
model = CNNModel(vocab_size, embedding_dim, num_classes)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training loop
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_preds = 0
    for inputs, masks, labels in train_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        correct_preds += (predicted == labels).sum().item()
        total_preds += labels.size(0)

    train_accuracy = correct_preds / total_preds * 100
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%")

# Evaluate on test set
model.eval()
y_true = []
y_pred = []
with torch.no_grad():
    for inputs, masks, labels in test_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Calculate accuracy, precision, recall, F1-score
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
print(f"Test Accuracy: {accuracy:.2f}%")
print(f"Test Precision: {precision:.2f}")
print(f"Test Recall: {recall:.2f}")
print(f"Test F1 Score: {f1:.2f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

"""###DS4"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer
import json
import ast

# Prepare dataset for CNN (ZSL Setup)
class DiseaseSymptomDataset(Dataset):
    def __init__(self, symptoms, labels, tokenizer, max_len=512):
        self.symptoms = symptoms
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.symptoms)

    def __getitem__(self, idx):
        symptom = self.symptoms[idx]
        label = self.labels[idx]
        # Tokenize symptoms using the tokenizer
        encoding = self.tokenizer(
            symptom,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()
        return input_ids, attention_mask, label


# CNN Model for Disease Prediction
class CNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes, kernel_sizes=[3, 4, 5], num_filters=128):
        super(CNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # Convolutional layers with different kernel sizes
        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filters, (size, embedding_dim)) for size in kernel_sizes
        ])

        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)

    def forward(self, x):
        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)
        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len, embedding_dim)

        # Apply convolutions for each kernel size
        conv_results = [conv(x) for conv in self.convs]
        conv_results = [torch.relu(res.squeeze(3)) for res in conv_results]  # Remove the last dim (embedding_dim)

        # Pooling (max pool over the sequence length)
        pooled_results = [torch.max(res, dim=2)[0] for res in conv_results]

        # Concatenate pooled results
        x = torch.cat(pooled_results, dim=1)
        x = self.fc(x)  # Final classification layer

        return x


# Data Preprocessing (handling symptom json column)
def extract_symptoms(messages):
    try:
        # Try parsing as JSON string first
        if isinstance(messages, str):
            try:
                messages_list = json.loads(messages)
            except:
                try:
                    messages_list = ast.literal_eval(messages)
                except:
                    # If both fail, try parsing with regex
                    symptoms = []
                    pattern = r"'role': 'patient', 'content': '(.+?)'"
                    matches = re.findall(pattern, messages)
                    return " ".join(matches)
        else:
            messages_list = messages

        # Now extract symptoms from parsed messages
        symptoms_text = ""
        for message in messages_list:
            if isinstance(message, dict) and message.get('role') == 'patient':
                symptoms_text += message.get('content', '') + " "
        return symptoms_text.strip()
    except Exception as e:
        print(f"Error processing message: {e}")
        return ""


# Load dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")
df = pd.DataFrame(ds['train'])

# Process the dataset
df['disease'] = df['prompt'].apply(lambda prompt: prompt.split("about")[-1].strip())
df['symptoms'] = df['messages'].apply(extract_symptoms)

# Drop rows with missing data
df = df.dropna(subset=['disease', 'symptoms'])
df = df[df['symptoms'] != ""]

# Encode disease labels
label_encoder = LabelEncoder()
df["disease_encoded"] = label_encoder.fit_transform(df["disease"])

# Split dataset (keep a small portion for ZSL)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Sample a few diseases for ZSL testing
zsl_train_df = train_df.groupby('disease').apply(lambda x: x.sample(n=3, random_state=42)).reset_index(drop=True)

# Tokenizer and DataLoader setup
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
train_dataset = DiseaseSymptomDataset(zsl_train_df["symptoms"].tolist(), zsl_train_df["disease_encoded"].tolist(), tokenizer)
test_dataset = DiseaseSymptomDataset(test_df["symptoms"].tolist(), test_df["disease_encoded"].tolist(), tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define the model
vocab_size = len(tokenizer)
embedding_dim = 768  # Use BERT-like embedding dimensions
num_classes = len(label_encoder.classes_)
model = CNNModel(vocab_size, embedding_dim, num_classes)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training loop (ZSL Setup)
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_preds = 0
    for inputs, masks, labels in train_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        correct_preds += (predicted == labels).sum().item()
        total_preds += labels.size(0)

    train_accuracy = correct_preds / total_preds * 100
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%")

# Evaluate on test set
model.eval()
y_true = []
y_pred = []
with torch.no_grad():
    for inputs, masks, labels in test_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Calculate accuracy, precision, recall, F1-score
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
print(f"Test Accuracy: {accuracy:.2f}%")
print(f"Test Precision: {precision:.2f}")
print(f"Test Recall: {recall:.2f}")
print(f"Test F1 Score: {f1:.2f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

"""##FSL

###DS1
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import random

# Prepare dataset for CNN (few-shot version)
class FewShotDiseaseDataset(Dataset):
    def __init__(self, symptoms, labels, tokenizer, max_len=512):
        self.symptoms = symptoms
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.symptoms)

    def __getitem__(self, idx):
        symptom = self.symptoms[idx]
        label = self.labels[idx]
        # Tokenize symptoms using the BioGPT tokenizer
        encoding = self.tokenizer(
            symptom,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()
        return input_ids, attention_mask, label


# CNN Model for Disease Prediction (FSL Approach)
class CNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes, kernel_sizes=[3, 4, 5], num_filters=128):
        super(CNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # Convolutional layers with different kernel sizes
        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filters, (size, embedding_dim)) for size in kernel_sizes
        ])

        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)

    def forward(self, x):
        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)
        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len, embedding_dim)

        # Apply convolutions for each kernel size
        conv_results = [conv(x) for conv in self.convs]
        conv_results = [torch.relu(res.squeeze(3)) for res in conv_results]  # Remove the last dim (embedding_dim)

        # Pooling (max pool over the sequence length)
        pooled_results = [torch.max(res, dim=2)[0] for res in conv_results]

        # Concatenate pooled results
        x = torch.cat(pooled_results, dim=1)
        x = self.fc(x)  # Final classification layer

        return x


# Preprocess the dataset
tokenizer = AutoTokenizer.from_pretrained("microsoft/biogpt")
df["symptoms"] = df["query"].str.extract(r'Patient:I may have (.*)')
df["disease"] = df["response"].str.replace("You may have ", "", regex=False)

# Drop missing values
df = df.dropna(subset=["symptoms", "disease"])

# Encoding diseases as labels
label_encoder = LabelEncoder()
df["disease_encoded"] = label_encoder.fit_transform(df["disease"])

# Select a few examples per disease class (Few-Shot Setup)
few_shot_size = 5  # You can adjust this value based on the number of examples per class you want
few_shot_df = df.groupby("disease_encoded").apply(lambda x: x.sample(min(few_shot_size, len(x)))).reset_index(drop=True)

# Split dataset into few-shot training and testing sets
train_df, test_df = train_test_split(few_shot_df, test_size=0.2, random_state=42)

# Create datasets and dataloaders for few-shot scenario
train_dataset = FewShotDiseaseDataset(train_df["symptoms"].tolist(), train_df["disease_encoded"].tolist(), tokenizer)
test_dataset = FewShotDiseaseDataset(test_df["symptoms"].tolist(), test_df["disease_encoded"].tolist(), tokenizer)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define the model, loss function, and optimizer
vocab_size = len(tokenizer)
embedding_dim = 768  # Use the output size of BioGPT's token embeddings
num_classes = len(label_encoder.classes_)
model = CNNModel(vocab_size, embedding_dim, num_classes)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Loss and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Fine-tuning the model with Few-Shot Learning (FSL)
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_preds = 0
    for inputs, masks, labels in train_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        correct_preds += (predicted == labels).sum().item()
        total_preds += labels.size(0)

    train_accuracy = correct_preds / total_preds * 100
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%")

# Evaluate on test set
model.eval()
y_true = []
y_pred = []
with torch.no_grad():
    for inputs, masks, labels in test_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Calculate accuracy, precision, recall, F1-score
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
print(f"Test Accuracy: {accuracy:.2f}%")
print(f"Test Precision: {precision:.2f}")
print(f"Test Recall: {recall:.2f}")
print(f"Test F1 Score: {f1:.2f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

"""###DS2"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
from datasets import load_dataset
from sklearn.model_selection import train_test_split
import re
from collections import Counter
from tqdm import tqdm
import random
from sklearn.utils import shuffle

# --------------------------
# Load & preprocess dataset
# --------------------------
ds = load_dataset("prognosis/symptoms_disease_v1")
df = pd.DataFrame(ds['train'])

processed = []
for _, row in df.iterrows():
    i, o = row['instruction'], row['output']
    s_match = re.search(r'I am having the following symptoms: (.*?)(?:\.|$)', i)
    d_match = re.search(r'dealing with (.*?)(?:\.|$)', o)
    if s_match and d_match:
        processed.append({"symptoms": s_match.group(1).strip().lower(), "disease": d_match.group(1).strip().lower()})

df = pd.DataFrame(processed)
label_encoder = LabelEncoder()
df["label"] = label_encoder.fit_transform(df["disease"])
all_diseases = df["disease"].unique()
random.shuffle(all_diseases)

# Split seen/unseen diseases
split_point = int(0.7 * len(all_diseases))
seen_diseases = all_diseases[:split_point]
unseen_diseases = all_diseases[split_point:]

train_df = df[df["disease"].isin(seen_diseases)].reset_index(drop=True)
test_df = df[df["disease"].isin(unseen_diseases)].reset_index(drop=True)

# --------------------------
# Tokenization and encoding
# --------------------------
def tokenize(text):
    return text.lower().split()

# Build vocab from training data only
token_counts = Counter()
for text in train_df["symptoms"]:
    token_counts.update(tokenize(text))

vocab = {word: i+2 for i, (word, _) in enumerate(token_counts.most_common())}
vocab["<PAD>"] = 0
vocab["<UNK>"] = 1

def encode(text, max_len=30):
    tokens = tokenize(text)
    ids = [vocab.get(t, vocab["<UNK>"]) for t in tokens]
    return ids[:max_len] + [0] * (max_len - len(ids[:max_len]))

# --------------------------
# Dataset and CNN Model
# --------------------------
class SymptomDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = [encode(t) for t in texts]
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])

class CNNEncoder(nn.Module):
    def __init__(self, vocab_size, embed_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.conv = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)
        self.pool = nn.AdaptiveMaxPool1d(1)

    def forward(self, x):
        x = self.embedding(x).transpose(1, 2)  # B x E x T
        x = F.relu(self.conv(x))
        x = self.pool(x).squeeze(-1)  # B x 128
        return x

# --------------------------
# Few-Shot Task Generation
# --------------------------
def create_few_shot_task(df, n_shots=5):
    """
    Creates a few-shot task by selecting n_shots samples for support and others for query.
    """
    task = {}
    diseases = df["disease"].unique()
    selected_disease = random.choice(diseases)
    task["disease"] = selected_disease
    disease_samples = df[df["disease"] == selected_disease]

    # Shuffle the disease samples to create support and query sets
    disease_samples = shuffle(disease_samples)
    support_set = disease_samples[:n_shots]
    query_set = disease_samples[n_shots:]

    task["support_set"] = support_set
    task["query_set"] = query_set
    return task

# --------------------------
# Fine-Tuning and Evaluation
# --------------------------

def train_few_shot_task(support_set, encoder, clf, opt, loss_fn, n_shots=5):
    encoder.train()
    clf.train()

    # Prepare data for the support set
    support_texts = support_set["symptoms"].tolist()
    support_labels = support_set["label"].tolist()
    support_ds = SymptomDataset(support_texts, support_labels)
    support_loader = DataLoader(support_ds, batch_size=n_shots, shuffle=True)

    total_loss = 0
    for x, y in support_loader:
        x, y = x.to(device), y.to(device)
        opt.zero_grad()
        emb = encoder(x)
        logits = clf(emb)
        loss = loss_fn(logits, y)
        loss.backward()
        opt.step()
        total_loss += loss.item()

    return total_loss

def evaluate_few_shot_task(query_set, encoder, clf, loss_fn):
    encoder.eval()
    clf.eval()

    query_texts = query_set["symptoms"].tolist()
    query_labels = query_set["label"].tolist()
    query_ds = SymptomDataset(query_texts, query_labels)
    query_loader = DataLoader(query_ds, batch_size=len(query_set), shuffle=False)

    correct = 0
    total = 0
    with torch.no_grad():
        for x, y in query_loader:
            x, y = x.to(device), y.to(device)
            emb = encoder(x)
            logits = clf(emb)
            pred = torch.argmax(logits, dim=1)
            correct += (pred == y).sum().item()
            total += len(y)

    return correct / total

# --------------------------
# Main Execution Loop
# --------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
embed_dim = 100
encoder = CNNEncoder(len(vocab), embed_dim).to(device)

clf = nn.Linear(128, len(set(df["label"]))).to(device)
opt = torch.optim.Adam(list(encoder.parameters()) + list(clf.parameters()), lr=1e-3)
loss_fn = nn.CrossEntropyLoss()

n_shots = 5  # Number of samples in support set per task
n_tasks = 100  # Number of tasks to simulate

for task_num in range(n_tasks):
    print(f"Task {task_num + 1}")

    # Create a few-shot task
    task = create_few_shot_task(train_df, n_shots=n_shots)

    # Train on the support set
    train_few_shot_task(task["support_set"], encoder, clf, opt, loss_fn, n_shots=n_shots)

    # Evaluate on the query set
    fsl_acc = evaluate_few_shot_task(task["query_set"], encoder, clf, loss_fn)

    print(f"Few-Shot Accuracy for Task {task_num + 1}: {fsl_acc:.2%}")

"""###DS3"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from transformers import AutoTokenizer
import ast

# Prepare dataset for CNN (Few-Shot Setup)
class DiseaseSymptomDataset(Dataset):
    def __init__(self, symptoms, labels, tokenizer, max_len=512):
        self.symptoms = symptoms
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.symptoms)

    def __getitem__(self, idx):
        symptom = self.symptoms[idx]
        label = self.labels[idx]
        # Tokenize symptoms using the tokenizer
        encoding = self.tokenizer(
            symptom,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()
        return input_ids, attention_mask, label


# CNN Model for Disease Prediction
class CNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes, kernel_sizes=[3, 4, 5], num_filters=128):
        super(CNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # Convolutional layers with different kernel sizes
        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filters, (size, embedding_dim)) for size in kernel_sizes
        ])

        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)

    def forward(self, x):
        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)
        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len, embedding_dim)

        # Apply convolutions for each kernel size
        conv_results = [conv(x) for conv in self.convs]
        conv_results = [torch.relu(res.squeeze(3)) for res in conv_results]  # Remove the last dim (embedding_dim)

        # Pooling (max pool over the sequence length)
        pooled_results = [torch.max(res, dim=2)[0] for res in conv_results]

        # Concatenate pooled results
        x = torch.cat(pooled_results, dim=1)
        x = self.fc(x)  # Final classification layer

        return x


# Data Preprocessing (handling symptom json column)
def extract_symptoms(symptom_json_str):
    try:
        # Parse the JSON string
        symptom_dict = ast.literal_eval(symptom_json_str)
        symptoms = symptom_dict.get('symptoms', [])
        return " | ".join(symptoms) if symptoms else ""
    except:
        return ""


# Load dataset
ds = load_dataset("Mohamed-Ahmed161/Disease-Symptoms")
df = pd.DataFrame(ds['train'])

# Extract symptoms text
df['symptoms_text'] = df['Symptom_json'].apply(extract_symptoms)
df = df[df['symptoms_text'] != ""]

# Encode disease labels
label_encoder = LabelEncoder()
df["Disease_Name_encoded"] = label_encoder.fit_transform(df["Disease_Name"])

# Split dataset (keep a small portion for FSL)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Select a small number of samples per class (Few-Shot Learning setup)
few_shot_train_df = train_df.groupby('Disease_Name').apply(lambda x: x.sample(n=3, random_state=42)).reset_index(drop=True)

# Initialize tokenizer and prepare datasets
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
train_dataset = DiseaseSymptomDataset(few_shot_train_df["symptoms_text"].tolist(), few_shot_train_df["Disease_Name_encoded"].tolist(), tokenizer)
test_dataset = DiseaseSymptomDataset(test_df["symptoms_text"].tolist(), test_df["Disease_Name_encoded"].tolist(), tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define the model
vocab_size = len(tokenizer)
embedding_dim = 768  # Use BERT-like embedding dimensions
num_classes = len(label_encoder.classes_)
model = CNNModel(vocab_size, embedding_dim, num_classes)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training loop (Few-Shot Learning)
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_preds = 0
    for inputs, masks, labels in train_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        correct_preds += (predicted == labels).sum().item()
        total_preds += labels.size(0)

    train_accuracy = correct_preds / total_preds * 100
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%")

# Evaluate on test set
model.eval()
y_true = []
y_pred = []
with torch.no_grad():
    for inputs, masks, labels in test_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Calculate accuracy, precision, recall, F1-score
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
print(f"Test Accuracy: {accuracy:.2f}%")
print(f"Test Precision: {precision:.2f}")
print(f"Test Recall: {recall:.2f}")
print(f"Test F1 Score: {f1:.2f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

"""###DS4"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer
import json
import ast

# Prepare dataset for CNN (FSL Setup)
class DiseaseSymptomDataset(Dataset):
    def __init__(self, symptoms, labels, tokenizer, max_len=512):
        self.symptoms = symptoms
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.symptoms)

    def __getitem__(self, idx):
        symptom = self.symptoms[idx]
        label = self.labels[idx]
        # Tokenize symptoms using the tokenizer
        encoding = self.tokenizer(
            symptom,
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()
        return input_ids, attention_mask, label


# CNN Model for Disease Prediction
class CNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, num_classes, kernel_sizes=[3, 4, 5], num_filters=128):
        super(CNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # Convolutional layers with different kernel sizes
        self.convs = nn.ModuleList([
            nn.Conv2d(1, num_filters, (size, embedding_dim)) for size in kernel_sizes
        ])

        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)

    def forward(self, x):
        x = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)
        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, seq_len, embedding_dim)

        # Apply convolutions for each kernel size
        conv_results = [conv(x) for conv in self.convs]
        conv_results = [torch.relu(res.squeeze(3)) for res in conv_results]  # Remove the last dim (embedding_dim)

        # Pooling (max pool over the sequence length)
        pooled_results = [torch.max(res, dim=2)[0] for res in conv_results]

        # Concatenate pooled results
        x = torch.cat(pooled_results, dim=1)
        x = self.fc(x)  # Final classification layer

        return x


# Data Preprocessing (handling symptom json column)
def extract_symptoms(messages):
    try:
        # Try parsing as JSON string first
        if isinstance(messages, str):
            try:
                messages_list = json.loads(messages)
            except:
                try:
                    messages_list = ast.literal_eval(messages)
                except:
                    # If both fail, try parsing with regex
                    symptoms = []
                    pattern = r"'role': 'patient', 'content': '(.+?)'"
                    matches = re.findall(pattern, messages)
                    return " ".join(matches)
        else:
            messages_list = messages

        # Now extract symptoms from parsed messages
        symptoms_text = ""
        for message in messages_list:
            if isinstance(message, dict) and message.get('role') == 'patient':
                symptoms_text += message.get('content', '') + " "
        return symptoms_text.strip()
    except Exception as e:
        print(f"Error processing message: {e}")
        return ""


# Load dataset
ds = load_dataset("Abhikhm/disease-symptoms-chat")
df = pd.DataFrame(ds['train'])

# Process the dataset
df['disease'] = df['prompt'].apply(lambda prompt: prompt.split("about")[-1].strip())
df['symptoms'] = df['messages'].apply(extract_symptoms)

# Drop rows with missing data
df = df.dropna(subset=['disease', 'symptoms'])
df = df[df['symptoms'] != ""]

# Encode disease labels
label_encoder = LabelEncoder()
df["disease_encoded"] = label_encoder.fit_transform(df["disease"])

# Split dataset (keep a small portion for FSL)
# Simulate Few-Shot Learning by selecting a few samples per class
fsl_train_df = df.groupby('disease').apply(lambda x: x.sample(n=3, random_state=42)).reset_index(drop=True)

# Tokenizer and DataLoader setup
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
train_dataset = DiseaseSymptomDataset(fsl_train_df["symptoms"].tolist(), fsl_train_df["disease_encoded"].tolist(), tokenizer)
test_dataset = DiseaseSymptomDataset(df["symptoms"].tolist(), df["disease_encoded"].tolist(), tokenizer)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Define the model
vocab_size = len(tokenizer)
embedding_dim = 768  # Use BERT-like embedding dimensions
num_classes = len(label_encoder.classes_)
model = CNNModel(vocab_size, embedding_dim, num_classes)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

# Training loop (FSL Setup)
num_epochs = 3
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_preds = 0
    for inputs, masks, labels in train_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        # Calculate accuracy
        _, predicted = torch.max(outputs, 1)
        correct_preds += (predicted == labels).sum().item()
        total_preds += labels.size(0)

    train_accuracy = correct_preds / total_preds * 100
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%")

# Evaluate on test set
model.eval()
y_true = []
y_pred = []
with torch.no_grad():
    for inputs, masks, labels in test_loader:
        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Calculate accuracy, precision, recall, F1-score
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')
print(f"Test Accuracy: {accuracy:.2f}%")
print(f"Test Precision: {precision:.2f}")
print(f"Test Recall: {recall:.2f}")
print(f"Test F1 Score: {f1:.2f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))